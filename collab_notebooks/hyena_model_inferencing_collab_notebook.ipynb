{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 50,
      "metadata": {
        "id": "y9-wvsGqEADW"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import numpy as np\n",
        "import psutil\n",
        "import logging\n",
        "import time\n",
        "import random\n",
        "from helical.models.hyena_dna import HyenaDNA, HyenaDNAConfig\n",
        "from tqdm import tqdm"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Download the HyenaDNAConfig model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zKUvni1mD_7R",
        "outputId": "4b0f6f14-b5a8-4834-d8ee-7ee3096a9e38"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2026-01-13 21:50:48,649 - INFO:helical.models.hyena_dna.pretrained_model:Loaded pretrained weights ok!\n",
            "2026-01-13 21:50:48,652 - INFO:helical.models.hyena_dna.model:Model finished initializing.\n",
            "2026-01-13 21:50:48,652 - INFO:helical.models.hyena_dna.model:'hyenadna-tiny-1k-seqlen-d256' model is in 'eval' mode, on device 'cpu'.\n"
          ]
        }
      ],
      "source": [
        "# Download the model using the hyena_config\n",
        "hyena_config = HyenaDNAConfig(\n",
        "    model_name = \"hyenadna-tiny-1k-seqlen-d256\"\n",
        ")\n",
        "model = HyenaDNA(configurer=hyena_config)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Setup Device string based on GPU availability"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "7z4VrJHbD_vx",
        "outputId": "ad2f4306-a750-4876-ea5a-f95351fe09e7"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'cpu'"
            ]
          },
          "execution_count": 20,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "DEVICE"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Move model to use GPU if possible"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "5X9kQ3a8anfM"
      },
      "outputs": [],
      "source": [
        "\n",
        "if DEVICE == \"cuda\":\n",
        "    model.model.to(DEVICE)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Download the dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 81,
          "referenced_widgets": [
            "90b2f19c56954dc3a06048d79852042d",
            "f2e2198a0b014e428aae57e67334239c",
            "2c4db2752a6b425497726e1e2317cf86",
            "b6d8ec80bbc8414b974b56d7052d85cf",
            "796af5db682b49408527e5d75450adac",
            "2c472a9566e04b07b1d0964810022a22",
            "c9b30f3dc6de4e2ca32ab7b40590fdd3",
            "dcad077b721843fabd025e1af19106dd",
            "a823c955f7b0411aad2a0d903a4f3b4d",
            "c7602a39c07843eaa75ba3af69d6fadb",
            "c453c20a0e3f4b5d8def4c9ef9bc4431",
            "715dddfbb691487192b3270d70ceb486",
            "39d989b0f6504e11b338dc7e7b2a90bb",
            "7e29a3c41cf34ac6a25837dd66559120",
            "13559fdf049c47628400f83d8989ac0c",
            "99291e8c19854c4494f3c0864d1ff9ee",
            "347b810802ef4aeaa15de9dc9309c69b",
            "4822183f0b504bc3b7de902c888f3143",
            "95832f724a4345059f2e82957824fef2",
            "688b915a56254c15a615691e6e84ebdf",
            "847ccbf4f15140deb91669f39ee709f6",
            "281b266c19bb4c32ac8defc4838c2d0f"
          ]
        },
        "id": "Lfm-tIaOD-6u",
        "outputId": "58c76b9c-48f6-4fd5-b45e-53aa7456926a"
      },
      "outputs": [],
      "source": [
        "from datasets import load_dataset\n",
        "label = \"promoter_tata\"\n",
        "\n",
        "dataset = load_dataset(\"InstaDeepAI/nucleotide_transformer_downstream_tasks\",  trust_remote_code=True).filter(lambda x: x[\"task\"] == \"promoter_tata\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Understanding the dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0BcnPM7aIP66",
        "outputId": "168c25e8-c4e3-4fda-f1f4-4e4891d608ed"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "DatasetDict({\n",
              "    train: Dataset({\n",
              "        features: ['sequence', 'name', 'label', 'task'],\n",
              "        num_rows: 5509\n",
              "    })\n",
              "    test: Dataset({\n",
              "        features: ['sequence', 'name', 'label', 'task'],\n",
              "        num_rows: 621\n",
              "    })\n",
              "})"
            ]
          },
          "execution_count": 25,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "id": "AuCMiWCXIQHM"
      },
      "outputs": [],
      "source": [
        "train_dataset = dataset[\"train\"]\n",
        "test_dataset = dataset[\"test\"]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "s_ssukEXIQL2",
        "outputId": "00e71ba7-2e07-46ad-a8e2-bd642f089ce1"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "{'sequence': Value(dtype='string', id=None),\n",
              " 'name': Value(dtype='string', id=None),\n",
              " 'label': Value(dtype='int32', id=None),\n",
              " 'task': Value(dtype='string', id=None)}"
            ]
          },
          "execution_count": 27,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "train_dataset.features"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Q--VWf-mIQPG",
        "outputId": "c126ee25-d7dc-4fa6-df97-0a8532ceb1d7"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "('CGCTCCCCCAGGAGTGTACTCCTGGTCAAAAGAGCGACATCACACGACGTAGGCCCGCCCGGCTTATCGAAGTCGAGCTGGGATTTGGGGGGGAACCTGACAGTATAGGTTGGGGGCCAGGACATTTATAGAACAACGGGAAAGACCTGCGCCAGCAGCTGAGAAGGAGGCCCCGTGATCAGCTCCAGCCATTTGCCAGCAACCGAAGCCCAGGAGCTTACATAATTTGCCAGGGCAGCACTGAGAGGTGACAGTTAGAGTTAAGTCGCTCTCGGAGCTCCGGGCTACCAGCGATTCTCT',\n",
              " 300,\n",
              " 5509)"
            ]
          },
          "execution_count": 28,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "sequences = train_dataset['sequence']\n",
        "sequences[0], len(sequences[0]), len(sequences)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Sampling the data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 45,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5R5Ix8y0M1vX",
        "outputId": "cbdcd5d2-4235-48c4-e816-8eda7a8bf66b"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(1000,\n",
              " 'CGCTCCCCCAGGAGTGTACTCCTGGTCAAAAGAGCGACATCACACGACGTAGGCCCGCCCGGCTTATCGAAGTCGAGCTGGGATTTGGGGGGGAACCTGACAGTATAGGTTGGGGGCCAGGACATTTATAGAACAACGGGAAAGACCTGCGCCAGCAGCTGAGAAGGAGGCCCCGTGATCAGCTCCAGCCATTTGCCAGCAACCGAAGCCCAGGAGCTTACATAATTTGCCAGGGCAGCACTGAGAGGTGACAGTTAGAGTTAAGTCGCTCTCGGAGCTCCGGGCTACCAGCGATTCTCT')"
            ]
          },
          "execution_count": 45,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "SAMPLE_SIZE = 1000\n",
        "sample_sequences = sequences[:SAMPLE_SIZE]\n",
        "len(sample_sequences), sample_sequences[0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 46,
      "metadata": {
        "id": "Okwb2uPsVJ0w"
      },
      "outputs": [],
      "source": [
        "# for suppressing INFO printing on each iteration\n",
        "logging.getLogger(\"helical.models.hyena_dna.model\").setLevel(logging.WARNING)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fq9-xreSkuGZ"
      },
      "source": [
        "## Task 1: Perturbation based inferencing (NAIVE)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 47,
      "metadata": {
        "id": "WbfGhlJRksn2"
      },
      "outputs": [],
      "source": [
        "def add_pertubations(sequence_string: str, num_of_pertubations: int):\n",
        "  \"\"\"adds pertubations to a sequence of nucleotides\"\"\"\n",
        "  nucleotides = [\"A\", \"G\", \"T\", \"C\"]\n",
        "  length = len(sequence_string)\n",
        "  seq_list = list(sequence_string)\n",
        "\n",
        "  for _ in range(num_of_pertubations):\n",
        "    # randomly choose index to perturb on\n",
        "    random_idx = np.random.randint(0, length - 1)\n",
        "\n",
        "    original_nucleotide = seq_list[random_idx]\n",
        "    \n",
        "    # find the possible perturbations\n",
        "    possible_pertubations = [n for n in nucleotides if n != original_nucleotide]\n",
        "    \n",
        "    # choose perturbation randomly out of possible_perturbations\n",
        "    new_nucleotide = random.choice(possible_pertubations)\n",
        "\n",
        "    # apply the pertubation to mutate\n",
        "    seq_list[random_idx] = new_nucleotide\n",
        "\n",
        "  # return perturbed sequence\n",
        "  return \"\".join(seq_list)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 48,
      "metadata": {
        "id": "5oaUBHGskshw"
      },
      "outputs": [],
      "source": [
        "preturbed_sequences = []\n",
        "\n",
        "for sequence in sample_sequences:\n",
        "    preturbed_sequences.append(add_pertubations(sequence, num_of_pertubations=1))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Run inferencing on perturbed sequences"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 51,
      "metadata": {
        "id": "As4ltCk8krkw"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 1000/1000 [00:43<00:00, 22.87it/s]\n"
          ]
        }
      ],
      "source": [
        "start = time.time()\n",
        "perturbed_embeddings = []\n",
        "latencies = []\n",
        "start_rss = psutil.Process().memory_info().rss / (1024 * 1024)\n",
        "BATCH_SIZE = 1\n",
        "overall_start = time.time()\n",
        "for i in tqdm(range(0, SAMPLE_SIZE, BATCH_SIZE)):\n",
        "  t_loop_in = time.time()\n",
        "  raw_tokens = model.process_data(preturbed_sequences[i:i + BATCH_SIZE])\n",
        "  input_ids_tensor = torch.tensor(raw_tokens[\"input_ids\"]).to(DEVICE)\n",
        "\n",
        "  with torch.no_grad():\n",
        "    outputs = model.model(input_ids=input_ids_tensor)\n",
        "    embeddings = outputs\n",
        "  t_loop_out = time.time()\n",
        "  latencies.append(t_loop_out - t_loop_in)\n",
        "  if isinstance(embeddings, torch.Tensor):\n",
        "    perturbed_embeddings.append(embeddings)\n",
        "overall_end = time.time()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 52,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "torch.Size([1000, 302, 256])"
            ]
          },
          "execution_count": 52,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "torch.cat(perturbed_embeddings, dim=0).shape"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Profiling the inferencing on perturbated sequences (NAIVE)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 53,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "------------ Profile : BATCH_SIZE = 1 (NAIVE) ------------\n",
            "Number of Samples processed: 1000\n",
            "Overall time taken to inference: 43.73\n",
            "Avg Latency per Batch: 43.61 ms\n",
            "Throughput Processed: 22.87 samples/s\n",
            "CPU RAM Usage: 34.640625 MB\n",
            "GPU Memory Peak: 0.00 MB\n",
            "---------------------------------------------------\n",
            "\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "import psutil\n",
        "# Profiling (without Batching)\n",
        "overall_time = overall_end - overall_start\n",
        "avg_latency = np.mean(latencies)\n",
        "throughput = SAMPLE_SIZE/overall_time\n",
        "end_rss = psutil.Process().memory_info().rss / (1024 * 1024)\n",
        "peak_gpu = torch.cuda.max_memory_allocated() / (1024 * 1024) if torch.cuda.is_available() else 0\n",
        "\n",
        "print(f\"\"\"\n",
        "------------ Profile : BATCH_SIZE = 1 (NAIVE) ------------\n",
        "Number of Samples processed: {len(preturbed_sequences)}\n",
        "Overall time taken to inference: {overall_time:.2f}\n",
        "Avg Latency per Batch: {1000*avg_latency:.2f} ms\n",
        "Throughput Processed: {throughput:.2f} samples/s\n",
        "CPU RAM Usage: {end_rss - start_rss:2f} MB\n",
        "GPU Memory Peak: {peak_gpu:.2f} MB\n",
        "---------------------------------------------------\n",
        "\"\"\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Task 2: Scale ISP"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Optimization 1 : Batching (Batch Size = 32)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 55,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 32/32 [00:21<00:00,  1.47it/s]\n"
          ]
        }
      ],
      "source": [
        "preturbed_sequences = []\n",
        "\n",
        "for sequence in sample_sequences:\n",
        "    preturbed_sequences.append(add_pertubations(sequence, num_of_pertubations=1))\n",
        "    \n",
        "start = time.time()\n",
        "perturbed_embeddings = []\n",
        "latencies = []\n",
        "start_rss = psutil.Process().memory_info().rss / (1024 * 1024)\n",
        "BATCH_SIZE = 32\n",
        "overall_start = time.time()\n",
        "for i in tqdm(range(0, SAMPLE_SIZE, BATCH_SIZE)):\n",
        "  t_loop_in = time.time()\n",
        "  raw_tokens = model.process_data(preturbed_sequences[i:i + BATCH_SIZE])\n",
        "  input_ids_tensor = torch.tensor(raw_tokens[\"input_ids\"]).to(DEVICE)\n",
        "\n",
        "  with torch.no_grad():\n",
        "    outputs = model.model(input_ids=input_ids_tensor)\n",
        "    embeddings = outputs\n",
        "  t_loop_out = time.time()\n",
        "  latencies.append(t_loop_out - t_loop_in)\n",
        "  if isinstance(embeddings, torch.Tensor):\n",
        "    perturbed_embeddings.append(embeddings)\n",
        "overall_end = time.time()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 56,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "------------ Profile : BATCH_SIZE = 32 ------------\n",
            "Number of Samples processed: 1000\n",
            "Overall time taken to inference: 21.78\n",
            "Avg Latency per Batch: 680.29 ms\n",
            "Throughput Processed: 45.91 samples/s\n",
            "CPU RAM Usage: 377.171875 MB\n",
            "GPU Memory Peak: 0.00 MB\n",
            "---------------------------------------------------\n",
            "\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "import psutil\n",
        "# Profiling (without Batching)\n",
        "overall_time = overall_end - overall_start\n",
        "avg_latency = np.mean(latencies)\n",
        "throughput = SAMPLE_SIZE/overall_time\n",
        "end_rss = psutil.Process().memory_info().rss / (1024 * 1024)\n",
        "peak_gpu = torch.cuda.max_memory_allocated() / (1024 * 1024) if torch.cuda.is_available() else 0\n",
        "\n",
        "print(f\"\"\"\n",
        "------------ Profile : BATCH_SIZE = 32 ------------\n",
        "Number of Samples processed: {len(preturbed_sequences)}\n",
        "Overall time taken to inference: {overall_time:.2f}\n",
        "Avg Latency per Batch: {1000*avg_latency:.2f} ms\n",
        "Throughput Processed: {throughput:.2f} samples/s\n",
        "CPU RAM Usage: {end_rss - start_rss:2f} MB\n",
        "GPU Memory Peak: {peak_gpu:.2f} MB\n",
        "---------------------------------------------------\n",
        "\"\"\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Optimization 2 : Mixed Precision"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 60,
      "metadata": {},
      "outputs": [],
      "source": [
        "from torch.amp import autocast"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 65,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 32/32 [00:18<00:00,  1.73it/s]\n"
          ]
        }
      ],
      "source": [
        "preturbed_sequences = []\n",
        "use_amp = True\n",
        "\n",
        "for sequence in sample_sequences:\n",
        "    preturbed_sequences.append(add_pertubations(sequence, num_of_pertubations=1))\n",
        "    \n",
        "overall_start = time.time()\n",
        "perturbed_embeddings = []\n",
        "latencies = []\n",
        "start_rss = psutil.Process().memory_info().rss / (1024 * 1024)\n",
        "BATCH_SIZE = 32\n",
        "overall_start = time.time()\n",
        "for i in tqdm(range(0, SAMPLE_SIZE, BATCH_SIZE)):\n",
        "    t_loop_in = time.time()\n",
        "    raw_tokens = model.process_data(preturbed_sequences[i:i + BATCH_SIZE])\n",
        "    input_ids_tensor = torch.tensor(raw_tokens[\"input_ids\"]).to(DEVICE)\n",
        "\n",
        "    with torch.no_grad():\n",
        "        with autocast(DEVICE, enabled=use_amp, dtype=torch.float16):\n",
        "            outputs = model.model(input_ids=input_ids_tensor)\n",
        "            embeddings = outputs\n",
        "        t_loop_out = time.time()\n",
        "        latencies.append(t_loop_out - t_loop_in)\n",
        "    if isinstance(embeddings, torch.Tensor):\n",
        "        perturbed_embeddings.append(embeddings)\n",
        "overall_end = time.time()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 66,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "------------ Profile : BATCH_SIZE = 32, Mixed Precision ------------\n",
            "Number of Samples processed: 1000\n",
            "Overall time taken to inference: 18.50\n",
            "Avg Latency per Batch: 577.73 ms\n",
            "Throughput Processed: 54.05 samples/s\n",
            "CPU RAM Usage: 466.703125 MB\n",
            "GPU Memory Peak: 0.00 MB\n",
            "---------------------------------------------------\n",
            "\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "import psutil\n",
        "# Profiling (without Batching)\n",
        "overall_time = overall_end - overall_start\n",
        "avg_latency = np.mean(latencies)\n",
        "throughput = SAMPLE_SIZE/overall_time\n",
        "end_rss = psutil.Process().memory_info().rss / (1024 * 1024)\n",
        "peak_gpu = torch.cuda.max_memory_allocated() / (1024 * 1024) if torch.cuda.is_available() else 0\n",
        "\n",
        "print(f\"\"\"\n",
        "------------ Profile : BATCH_SIZE = 32, Mixed Precision ------------\n",
        "Number of Samples processed: {len(preturbed_sequences)}\n",
        "Overall time taken to inference: {overall_time:.2f}\n",
        "Avg Latency per Batch: {1000*avg_latency:.2f} ms\n",
        "Throughput Processed: {throughput:.2f} samples/s\n",
        "CPU RAM Usage: {end_rss - start_rss:2f} MB\n",
        "GPU Memory Peak: {peak_gpu:.2f} MB\n",
        "---------------------------------------------------\n",
        "\"\"\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": ".helical_conda_venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.13"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "13559fdf049c47628400f83d8989ac0c": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_847ccbf4f15140deb91669f39ee709f6",
            "placeholder": "​",
            "style": "IPY_MODEL_281b266c19bb4c32ac8defc4838c2d0f",
            "value": " 18/18 [00:00&lt;00:00, 119.46it/s]"
          }
        },
        "281b266c19bb4c32ac8defc4838c2d0f": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "2c472a9566e04b07b1d0964810022a22": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "2c4db2752a6b425497726e1e2317cf86": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_dcad077b721843fabd025e1af19106dd",
            "max": 18,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_a823c955f7b0411aad2a0d903a4f3b4d",
            "value": 18
          }
        },
        "347b810802ef4aeaa15de9dc9309c69b": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "39d989b0f6504e11b338dc7e7b2a90bb": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_347b810802ef4aeaa15de9dc9309c69b",
            "placeholder": "​",
            "style": "IPY_MODEL_4822183f0b504bc3b7de902c888f3143",
            "value": "Resolving data files: 100%"
          }
        },
        "4822183f0b504bc3b7de902c888f3143": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "688b915a56254c15a615691e6e84ebdf": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "715dddfbb691487192b3270d70ceb486": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_39d989b0f6504e11b338dc7e7b2a90bb",
              "IPY_MODEL_7e29a3c41cf34ac6a25837dd66559120",
              "IPY_MODEL_13559fdf049c47628400f83d8989ac0c"
            ],
            "layout": "IPY_MODEL_99291e8c19854c4494f3c0864d1ff9ee"
          }
        },
        "796af5db682b49408527e5d75450adac": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "7e29a3c41cf34ac6a25837dd66559120": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_95832f724a4345059f2e82957824fef2",
            "max": 18,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_688b915a56254c15a615691e6e84ebdf",
            "value": 18
          }
        },
        "847ccbf4f15140deb91669f39ee709f6": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "90b2f19c56954dc3a06048d79852042d": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_f2e2198a0b014e428aae57e67334239c",
              "IPY_MODEL_2c4db2752a6b425497726e1e2317cf86",
              "IPY_MODEL_b6d8ec80bbc8414b974b56d7052d85cf"
            ],
            "layout": "IPY_MODEL_796af5db682b49408527e5d75450adac"
          }
        },
        "95832f724a4345059f2e82957824fef2": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "99291e8c19854c4494f3c0864d1ff9ee": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "a823c955f7b0411aad2a0d903a4f3b4d": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "b6d8ec80bbc8414b974b56d7052d85cf": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_c7602a39c07843eaa75ba3af69d6fadb",
            "placeholder": "​",
            "style": "IPY_MODEL_c453c20a0e3f4b5d8def4c9ef9bc4431",
            "value": " 18/18 [00:00&lt;00:00, 104.71it/s]"
          }
        },
        "c453c20a0e3f4b5d8def4c9ef9bc4431": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "c7602a39c07843eaa75ba3af69d6fadb": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "c9b30f3dc6de4e2ca32ab7b40590fdd3": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "dcad077b721843fabd025e1af19106dd": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "f2e2198a0b014e428aae57e67334239c": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_2c472a9566e04b07b1d0964810022a22",
            "placeholder": "​",
            "style": "IPY_MODEL_c9b30f3dc6de4e2ca32ab7b40590fdd3",
            "value": "Resolving data files: 100%"
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
