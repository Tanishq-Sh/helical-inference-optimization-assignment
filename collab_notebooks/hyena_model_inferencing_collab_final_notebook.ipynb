{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Uncomment and run following lines to install dependencies and restart the kernel\n",
        "# !pip install helical torch torchvision\n",
        "# !pip install numpy==2.2.5"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "y9-wvsGqEADW"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2026-01-14 15:02:12,323 - WARNING:py.warnings:/opt/homebrew/Caskroom/miniforge/base/envs/.helical_conda_venv/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
            "  from .autonotebook import tqdm as notebook_tqdm\n",
            "\n",
            "2026-01-14 15:02:12,370 - INFO:datasets:PyTorch version 2.7.0 available.\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "from torch.amp import autocast\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import psutil\n",
        "import time\n",
        "import random\n",
        "from helical.models.hyena_dna import HyenaDNA, HyenaDNAConfig\n",
        "from datasets import load_dataset\n",
        "from tqdm import tqdm\n",
        "import logging\n",
        "\n",
        "logging.getLogger(\"helical.models.hyena_dna.model\").setLevel(logging.WARNING)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Task 1 : Profiling on Naive Inferencing"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Configurations to set before inferencing (for naive inferencing)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\" # for dynamic switching between CPU and GPU based on availability \n",
        "\n",
        "MODEL_CONFIG = {\n",
        "    \"model_name\": \"hyenadna-tiny-1k-seqlen-d256\", # Model name to use\n",
        "    # \"model_name\": \"hyenadna-tiny-1k-seqlen\", # Alternate Model name to use\n",
        "    \"batch_size\": 1, # Number of sequences to process in a batch\n",
        "    \"use_amp\": False, # Use auto mixed precision for optimization\n",
        "    \"amp_dtype\": torch.float16 # will use amp_dtype only when use_amp is set to True\n",
        "}\n",
        "\n",
        "DATA_CONFIG = {\n",
        "    \"sample_size\": 50, # Change if we want a different sample size\n",
        "    \"number_of_perturbations\": 1 # Change if we want more perturbations\n",
        "}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [],
      "source": [
        "# for benchmarking all the experiments\n",
        "all_profiles = []\n",
        "\n",
        "def load_hyena_model():\n",
        "    \"\"\"\n",
        "    Loads the HyenaDNA model based on settings in config.py,\n",
        "    moves it to the correct device, and returns the model object.\n",
        "    \"\"\"\n",
        "    hyena_config = HyenaDNAConfig(\n",
        "        model_name = MODEL_CONFIG[\"model_name\"], # HyenaDNA models can be used here\n",
        "    )\n",
        "    model = HyenaDNA(configurer=hyena_config)\n",
        "\n",
        "    # move model to use GPU if possible\n",
        "    if DEVICE == \"cuda\":\n",
        "        model.model.to(DEVICE)\n",
        "        \n",
        "    print(\"Model loaded successfully!\")\n",
        "    \n",
        "    return model\n",
        "  \n",
        "def compare_embeddings(embedding1, embedding2, name1=\"Embedding 1\", name2=\"Embedding 2\"):\n",
        "    \"\"\"\n",
        "    Compares two embedding tensors by calculating Mean Absolute Error and Cosine Similarity.\n",
        "\n",
        "    Args:\n",
        "        embedding1 (torch.Tensor): The first embedding tensor.\n",
        "        embedding2 (torch.Tensor): The second embedding tensor.\n",
        "        name1 (str): Name for the first embedding for printing.\n",
        "        name2 (str): Name for the second embedding for printing.\n",
        "    \"\"\"\n",
        "    # Ensure tensors are on the same device (CPU) for comparison and are float32\n",
        "    emb1_cpu = embedding1.cpu().to(torch.float32)\n",
        "    emb2_cpu = embedding2.cpu().to(torch.float32)\n",
        "\n",
        "    # 1. Calculate Mean Absolute Error (MAE)\n",
        "    mae = torch.mean(torch.abs(emb1_cpu - emb2_cpu))\n",
        "\n",
        "    # 2. Calculate Cosine Similarity\n",
        "    # We compute it for each pair of vectors (row-wise) and then average\n",
        "    cos_sim = torch.nn.functional.cosine_similarity(emb1_cpu, emb2_cpu, dim=1)\n",
        "    avg_cos_sim = torch.mean(cos_sim)\n",
        "\n",
        "    print(f\"--- Embedding Comparison: {name1} vs. {name2} ---\")\n",
        "    print(f\"Mean Absolute Error:     {mae.item():.10f}\")\n",
        "    print(f\"Average Cosine Similarity: {avg_cos_sim.item():.10f}\")\n",
        "\n",
        "  \n",
        "def get_sequences(sample_size: int):\n",
        "    \"\"\"Download the promoter_tata dataset and returns a sample of sequences\n",
        "\n",
        "    Args:\n",
        "        sample_size (int): Number of samples we want to use\n",
        "    \"\"\"\n",
        "    print(\"Downloading dataset ...\")\n",
        "    dataset = load_dataset(\n",
        "        \"InstaDeepAI/nucleotide_transformer_downstream_tasks\",\n",
        "        trust_remote_code=True\n",
        "    ).filter(lambda x: x[\"task\"] == \"promoter_tata\")\n",
        "    \n",
        "    sequences = dataset[\"train\"][\"sequence\"]\n",
        "    print(f\"Dataset is loaded, we will be taking a sample of {sample_size}\")\n",
        "    return sequences[:sample_size]\n",
        "  \n",
        "\n",
        "def add_pertubations(sequence_string: str, num_of_pertubations: int):\n",
        "  \"\"\"adds pertubations to a sequence of nucleotides\"\"\"\n",
        "  nucleotides = [\"A\", \"G\", \"T\", \"C\"]\n",
        "  length = len(sequence_string)\n",
        "  seq_list = list(sequence_string)\n",
        "\n",
        "  for _ in range(num_of_pertubations):\n",
        "    random_idx = np.random.randint(0, length - 1)\n",
        "\n",
        "    original_nucleotide = seq_list[random_idx]\n",
        "    possible_pertubations = [n for n in nucleotides if n != original_nucleotide]\n",
        "    new_nucleotide = random.choice(possible_pertubations)\n",
        "\n",
        "    # apply the pertubation to mutate\n",
        "    seq_list[random_idx] = new_nucleotide\n",
        "\n",
        "  # return perturbed sequence\n",
        "  return \"\".join(seq_list)\n",
        "\n",
        "def log_inference_profile(\n",
        "  total_time: float,\n",
        "  latencies: list,\n",
        "  num_samples: int,\n",
        "  start_rss_mb: float\n",
        "):\n",
        "  \"\"\"\n",
        "  Calculates and logs inference related metrics of the run\n",
        "  \"\"\"\n",
        "  avg_latency = np.mean(latencies) * 1000 # to convert in ms\n",
        "  throughput = DATA_CONFIG[\"sample_size\"]/total_time\n",
        "  end_rss_mb = psutil.Process().memory_info().rss / (1024 * 1024)\n",
        "  \n",
        "  # get GPU memory if running on GPU\n",
        "  peak_gpu_mb = 0\n",
        "  if DEVICE == \"cuda\" and torch.cuda.is_available():\n",
        "    peak_gpu_mb = torch.cuda.max_memory_allocated() / (1024 * 1024)\n",
        "    \n",
        "  log_line = f\"\"\"\n",
        "------------ Inference Profile ------------\n",
        "Device:                 {DEVICE.upper()}\n",
        "Model name:             {MODEL_CONFIG[\"model_name\"]}\n",
        "amp enabled:            {MODEL_CONFIG[\"use_amp\"]}\n",
        "amp dtype:              {MODEL_CONFIG[\"amp_dtype\"] if MODEL_CONFIG[\"use_amp\"] else \"torch.float32 (default)\"}\n",
        "Total Samples:          {num_samples}\n",
        "Batch Size:             {MODEL_CONFIG[\"batch_size\"]}\n",
        "---\n",
        "Total Time:             {total_time:.2f} s\n",
        "Throughput:             {throughput:.2f} samples/s\n",
        "Avg. Latency / Batch:   {avg_latency:.2f} ms\n",
        "---\n",
        "CPU RAM Usage:          {end_rss_mb - start_rss_mb:.2f} MB\n",
        "Peak GPU Memory:        {peak_gpu_mb:.2f} MB\n",
        "---------------------------------------------------\n",
        "\"\"\"\n",
        "  # Create a dictionary of the results\n",
        "  profile_results = {\n",
        "      \"Model name\":  MODEL_CONFIG[\"model_name\"],\n",
        "      \"Total samples\": num_samples,\n",
        "      \"Total time\": f\"{total_time:.2f} s\",\n",
        "      \"Throughput (samples/s)\": f\"{throughput:.2f}\",\n",
        "      \"Avg. Latency (ms/batch)\": f\"{avg_latency:.2f}\",\n",
        "      \"CPU RAM Usage (MB)\": f\"{end_rss_mb - start_rss_mb:.2f}\",\n",
        "      \"Peak GPU Memory (MB)\": f\"{peak_gpu_mb:.2f}\"\n",
        "  }\n",
        "  \n",
        "  # Reset peak memory stats for the next run if needed\n",
        "  if DEVICE == \"cuda\" and torch.cuda.is_available():\n",
        "      torch.cuda.reset_peak_memory_stats()\n",
        "      \n",
        "  return log_line, profile_results\n",
        "\n",
        "\n",
        "def run_hyena_inferencing(model, sequences_to_process: list):\n",
        "    \"\"\"runs inferencing on sequences\"\"\"\n",
        "    pertubation_embeddings = []\n",
        "    latencies = []\n",
        "\n",
        "    # Inference on pertubations\n",
        "    BATCH_SIZE = MODEL_CONFIG[\"batch_size\"]\n",
        "    overall_start = time.time()\n",
        "    start_rss = psutil.Process().memory_info().rss / (1024 * 1024)\n",
        "\n",
        "    for i in tqdm(range(0, DATA_CONFIG[\"sample_size\"], BATCH_SIZE)):\n",
        "        t_loop_in = time.time()\n",
        "        raw_tokens = model.process_data(sequences_to_process[i:i + BATCH_SIZE])\n",
        "        input_ids_tensor = torch.tensor(raw_tokens[\"input_ids\"]).to(DEVICE)\n",
        "\n",
        "        with torch.no_grad():\n",
        "            with autocast(DEVICE, enabled=MODEL_CONFIG[\"use_amp\"], dtype=MODEL_CONFIG[\"amp_dtype\"]):\n",
        "                outputs = model.model(input_ids=input_ids_tensor)\n",
        "                embeddings = outputs\n",
        "\n",
        "        t_loop_out = time.time()\n",
        "        latencies.append(t_loop_out - t_loop_in)\n",
        "        \n",
        "        if isinstance(embeddings, torch.Tensor):\n",
        "            pertubation_embeddings.append(embeddings)\n",
        "\n",
        "    total_time = time.time() - overall_start\n",
        "\n",
        "    # Call the logging function from utils\n",
        "    run_profile, run_profile_dict = log_inference_profile(\n",
        "        total_time=total_time,\n",
        "        latencies=latencies,\n",
        "        num_samples=len(sequences_to_process),\n",
        "        start_rss_mb=start_rss\n",
        "    )\n",
        "    \n",
        "    print(run_profile)\n",
        "\n",
        "    return torch.cat(pertubation_embeddings, dim=0), run_profile_dict"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Profiling the inferencing on perturbated sequences (NAIVE)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Downloading dataset ...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2026-01-14 15:02:16,476 - INFO:helical.models.hyena_dna.pretrained_model:Loaded pretrained weights ok!\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Dataset is loaded, we will be taking a sample of 500\n",
            "Loading Hyena model...\n",
            "Model loaded successfully!\n",
            "Starting inference run on original sequences...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 500/500 [00:20<00:00, 24.31it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "------------ Inference Profile ------------\n",
            "Device:                 CPU\n",
            "Model name:             hyenadna-tiny-1k-seqlen-d256\n",
            "amp enabled:            False\n",
            "amp dtype:              torch.float32 (default)\n",
            "Total Samples:          500\n",
            "Batch Size:             1\n",
            "---\n",
            "Total Time:             20.57 s\n",
            "Throughput:             24.31 samples/s\n",
            "Avg. Latency / Batch:   41.01 ms\n",
            "---\n",
            "CPU RAM Usage:          -206.17 MB\n",
            "Peak GPU Memory:        0.00 MB\n",
            "---------------------------------------------------\n",
            "\n",
            "Starting inference run on perturbed sequences...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 500/500 [00:20<00:00, 24.43it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "------------ Inference Profile ------------\n",
            "Device:                 CPU\n",
            "Model name:             hyenadna-tiny-1k-seqlen-d256\n",
            "amp enabled:            False\n",
            "amp dtype:              torch.float32 (default)\n",
            "Total Samples:          500\n",
            "Batch Size:             1\n",
            "---\n",
            "Total Time:             20.47 s\n",
            "Throughput:             24.42 samples/s\n",
            "Avg. Latency / Batch:   40.83 ms\n",
            "---\n",
            "CPU RAM Usage:          52.56 MB\n",
            "Peak GPU Memory:        0.00 MB\n",
            "---------------------------------------------------\n",
            "\n",
            "Successfully generated original embeddings of shape: torch.Size([500, 302, 256])\n",
            "Successfully generated perturbed embeddings of shape: torch.Size([500, 302, 256])\n"
          ]
        }
      ],
      "source": [
        "time_signature = time.strftime('%Y%m%d-%H%M%S')\n",
        "\n",
        "sequences = get_sequences(DATA_CONFIG[\"sample_size\"])\n",
        "\n",
        "perturbed_sequences = []\n",
        "\n",
        "for sequence in sequences:\n",
        "    perturbed_sequences.append(add_pertubations(sequence, num_of_pertubations=1))\n",
        "\n",
        "perturbed_sequences = [add_pertubations(seq, num_of_pertubations=DATA_CONFIG[\"number_of_perturbations\"]) for seq in sequences]\n",
        "\n",
        "print(\"Loading Hyena model...\")\n",
        "model = load_hyena_model()\n",
        "\n",
        "print(\"Starting inference run on original sequences...\")\n",
        "naive_original_embeddings, naive_original_profile = run_hyena_inferencing(model, sequences)\n",
        "naive_original_profile[\"Sequences\"] = \"Original\"\n",
        "naive_original_profile[\"Experiment\"] = \"Naive (BS=1, FP32)\"\n",
        "all_profiles.append(naive_original_profile)\n",
        "\n",
        "print(\"Starting inference run on perturbed sequences...\")\n",
        "naive_perturbed_embeddings, naive_perturbed_profile = run_hyena_inferencing(model, perturbed_sequences)\n",
        "naive_perturbed_profile[\"Sequences\"] = \"Perturbed\"\n",
        "naive_perturbed_profile[\"Experiment\"] = \"Naive (BS=1, FP32)\"\n",
        "all_profiles.append(naive_perturbed_profile)\n",
        "\n",
        "\n",
        "print(f\"Successfully generated original embeddings of shape: {naive_original_embeddings.shape}\")\n",
        "print(f\"Successfully generated perturbed embeddings of shape: {naive_perturbed_embeddings.shape}\")\n",
        "\n",
        "original_embedding_array = np.stack(\n",
        "    [emb.cpu().numpy() for emb in naive_original_embeddings]\n",
        ")\n",
        "\n",
        "perturbed_embedding_array = np.stack(\n",
        "    [emb.cpu().numpy() for emb in naive_perturbed_embeddings]\n",
        ")\n",
        "\n",
        "np.save(f\"original_embedding_{time_signature}.npy\", original_embedding_array)\n",
        "np.save(f\"perturbed_embedding_{time_signature}.npy\", original_embedding_array)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Task 2: Scale ISP and Optimizations"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Optimization 1 : Batching (Batch Size = 32)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Configurations to set before inferencing (for batching optimization)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [],
      "source": [
        "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\" # for dynamic switching between CPU and GPU based on availability \n",
        "\n",
        "MODEL_CONFIG = {\n",
        "    \"model_name\": \"hyenadna-tiny-1k-seqlen-d256\", # Model name to use\n",
        "    # \"model_name\": \"hyenadna-tiny-1k-seqlen\", # Alternate Model name to use\n",
        "    \"batch_size\": 32, # Number of sequences to process in a batch\n",
        "    \"use_amp\": False, # Use auto mixed precision for optimization\n",
        "    \"amp_dtype\": torch.float16 # will use amp_dtype only when use_amp is set to True\n",
        "}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Downloading dataset ...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2026-01-14 15:02:59,250 - INFO:helical.models.hyena_dna.pretrained_model:Loaded pretrained weights ok!\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Dataset is loaded, we will be taking a sample of 500\n",
            "Loading Hyena model...\n",
            "Model loaded successfully!\n",
            "Starting inference run on original sequences...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 16/16 [00:11<00:00,  1.45it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "------------ Inference Profile ------------\n",
            "Device:                 CPU\n",
            "Model name:             hyenadna-tiny-1k-seqlen-d256\n",
            "amp enabled:            False\n",
            "amp dtype:              torch.float32 (default)\n",
            "Total Samples:          500\n",
            "Batch Size:             32\n",
            "---\n",
            "Total Time:             11.01 s\n",
            "Throughput:             45.41 samples/s\n",
            "Avg. Latency / Batch:   687.70 ms\n",
            "---\n",
            "CPU RAM Usage:          252.94 MB\n",
            "Peak GPU Memory:        0.00 MB\n",
            "---------------------------------------------------\n",
            "\n",
            "Starting inference run on perturbed sequences...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 16/16 [00:11<00:00,  1.41it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "------------ Inference Profile ------------\n",
            "Device:                 CPU\n",
            "Model name:             hyenadna-tiny-1k-seqlen-d256\n",
            "amp enabled:            False\n",
            "amp dtype:              torch.float32 (default)\n",
            "Total Samples:          500\n",
            "Batch Size:             32\n",
            "---\n",
            "Total Time:             11.38 s\n",
            "Throughput:             43.95 samples/s\n",
            "Avg. Latency / Batch:   710.49 ms\n",
            "---\n",
            "CPU RAM Usage:          -95.02 MB\n",
            "Peak GPU Memory:        0.00 MB\n",
            "---------------------------------------------------\n",
            "\n",
            "Successfully generated original embeddings of shape: torch.Size([500, 302, 256])\n",
            "Successfully generated perturbed embeddings of shape: torch.Size([500, 302, 256])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "time_signature = time.strftime('%Y%m%d-%H%M%S')\n",
        "\n",
        "sequences = get_sequences(DATA_CONFIG[\"sample_size\"])\n",
        "\n",
        "perturbed_sequences = []\n",
        "\n",
        "for sequence in sequences:\n",
        "    perturbed_sequences.append(add_pertubations(sequence, num_of_pertubations=1))\n",
        "\n",
        "perturbed_sequences = [add_pertubations(seq, num_of_pertubations=DATA_CONFIG[\"number_of_perturbations\"]) for seq in sequences]\n",
        "\n",
        "print(\"Loading Hyena model...\")\n",
        "model = load_hyena_model()\n",
        "\n",
        "print(\"Starting inference run on original sequences...\")\n",
        "batch_only_original_embeddings, optimization_one_original_profile = run_hyena_inferencing(model, sequences)\n",
        "optimization_one_original_profile[\"Sequences\"] = \"Original\"\n",
        "optimization_one_original_profile[\"Experiment\"] = \"Batching Only (BS=32, FP32)\"\n",
        "all_profiles.append(optimization_one_original_profile)\n",
        "\n",
        "print(\"Starting inference run on perturbed sequences...\")\n",
        "batch_only_perturbed_embeddings, optimization_one_perturbed_profile = run_hyena_inferencing(model, perturbed_sequences)\n",
        "optimization_one_perturbed_profile[\"Sequences\"] = \"Perturbed\"\n",
        "optimization_one_perturbed_profile[\"Experiment\"] = \"Batching Only (BS=32, FP32)\"\n",
        "all_profiles.append(optimization_one_perturbed_profile)\n",
        "\n",
        "print(f\"Successfully generated original embeddings of shape: {batch_only_original_embeddings.shape}\")\n",
        "print(f\"Successfully generated perturbed embeddings of shape: {batch_only_perturbed_embeddings.shape}\")\n",
        "\n",
        "original_embedding_array = np.stack(\n",
        "    [emb.cpu().numpy() for emb in batch_only_original_embeddings]\n",
        ")\n",
        "\n",
        "perturbed_embedding_array = np.stack(\n",
        "    [emb.cpu().numpy() for emb in batch_only_perturbed_embeddings]\n",
        ")\n",
        "\n",
        "np.save(f\"original_embedding_{time_signature}.npy\", original_embedding_array)\n",
        "np.save(f\"perturbed_embedding_{time_signature}.npy\", original_embedding_array)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Optimization 2 : Mixed Precision (Batch Size = 32)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Configurations to set before inferencing (for batching optimization)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [],
      "source": [
        "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\" # for dynamic switching between CPU and GPU based on availability \n",
        "\n",
        "MODEL_CONFIG = {\n",
        "    \"model_name\": \"hyenadna-tiny-1k-seqlen-d256\", # Model name to use\n",
        "    # \"model_name\": \"hyenadna-tiny-1k-seqlen\", # Alternate Model name to use\n",
        "    \"batch_size\": 32, # Number of sequences to process in a batch\n",
        "    \"use_amp\": True, # Use auto mixed precision for optimization\n",
        "    \"amp_dtype\": torch.float16 # will use amp_dtype only when use_amp is set to True\n",
        "}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Downloading dataset ...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2026-01-14 15:03:23,597 - INFO:helical.models.hyena_dna.pretrained_model:Loaded pretrained weights ok!\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Dataset is loaded, we will be taking a sample of 500\n",
            "Loading Hyena model...\n",
            "Model loaded successfully!\n",
            "Starting inference run on original sequences...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 16/16 [00:10<00:00,  1.58it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "------------ Inference Profile ------------\n",
            "Device:                 CPU\n",
            "Model name:             hyenadna-tiny-1k-seqlen-d256\n",
            "amp enabled:            True\n",
            "amp dtype:              torch.float16\n",
            "Total Samples:          500\n",
            "Batch Size:             32\n",
            "---\n",
            "Total Time:             10.16 s\n",
            "Throughput:             49.24 samples/s\n",
            "Avg. Latency / Batch:   634.19 ms\n",
            "---\n",
            "CPU RAM Usage:          67.09 MB\n",
            "Peak GPU Memory:        0.00 MB\n",
            "---------------------------------------------------\n",
            "\n",
            "Starting inference run on perturbed sequences...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 16/16 [00:10<00:00,  1.57it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "------------ Inference Profile ------------\n",
            "Device:                 CPU\n",
            "Model name:             hyenadna-tiny-1k-seqlen-d256\n",
            "amp enabled:            True\n",
            "amp dtype:              torch.float16\n",
            "Total Samples:          500\n",
            "Batch Size:             32\n",
            "---\n",
            "Total Time:             10.17 s\n",
            "Throughput:             49.17 samples/s\n",
            "Avg. Latency / Batch:   635.08 ms\n",
            "---\n",
            "CPU RAM Usage:          -514.53 MB\n",
            "Peak GPU Memory:        0.00 MB\n",
            "---------------------------------------------------\n",
            "\n",
            "Successfully generated original embeddings of shape: torch.Size([500, 302, 256])\n",
            "Successfully generated perturbed embeddings of shape: torch.Size([500, 302, 256])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "time_signature = time.strftime('%Y%m%d-%H%M%S')\n",
        "\n",
        "sequences = get_sequences(DATA_CONFIG[\"sample_size\"])\n",
        "\n",
        "perturbed_sequences = []\n",
        "\n",
        "for sequence in sequences:\n",
        "    perturbed_sequences.append(add_pertubations(sequence, num_of_pertubations=1))\n",
        "\n",
        "perturbed_sequences = [add_pertubations(seq, num_of_pertubations=DATA_CONFIG[\"number_of_perturbations\"]) for seq in sequences]\n",
        "\n",
        "print(\"Loading Hyena model...\")\n",
        "model = load_hyena_model()\n",
        "\n",
        "print(\"Starting inference run on original sequences...\")\n",
        "batch_and_amp_original_embeddings, optimization_two_original_profile = run_hyena_inferencing(model, sequences)\n",
        "optimization_two_original_profile[\"Sequences\"] = \"Original\"\n",
        "optimization_two_original_profile[\"Experiment\"] = \"Batching with AMP (BS=32, FP16)\"\n",
        "all_profiles.append(optimization_two_original_profile)\n",
        "\n",
        "print(\"Starting inference run on perturbed sequences...\")\n",
        "batch_and_amp_perturbed_embeddings, optimization_two_perturbed_profile = run_hyena_inferencing(model, perturbed_sequences)\n",
        "optimization_two_perturbed_profile[\"Sequences\"] = \"Perturbed\"\n",
        "optimization_two_perturbed_profile[\"Experiment\"] = \"Batching with AMP (BS=32, FP16)\"\n",
        "all_profiles.append(optimization_two_perturbed_profile)\n",
        "\n",
        "print(f\"Successfully generated original embeddings of shape: {batch_and_amp_original_embeddings.shape}\")\n",
        "print(f\"Successfully generated perturbed embeddings of shape: {batch_and_amp_perturbed_embeddings.shape}\")\n",
        "\n",
        "original_embedding_array = np.stack(\n",
        "    [emb.cpu().numpy() for emb in batch_and_amp_original_embeddings]\n",
        ")\n",
        "\n",
        "perturbed_embedding_array = np.stack(\n",
        "    [emb.cpu().numpy() for emb in batch_and_amp_perturbed_embeddings]\n",
        ")\n",
        "\n",
        "np.save(f\"original_embedding_{time_signature}.npy\", original_embedding_array)\n",
        "np.save(f\"perturbed_embedding_{time_signature}.npy\", original_embedding_array)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Summarizing Results"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "application/vnd.microsoft.datawrangler.viewer.v0+json": {
              "columns": [
                {
                  "name": "index",
                  "rawType": "int64",
                  "type": "integer"
                },
                {
                  "name": "Model name",
                  "rawType": "object",
                  "type": "string"
                },
                {
                  "name": "Total samples",
                  "rawType": "int64",
                  "type": "integer"
                },
                {
                  "name": "Total time",
                  "rawType": "object",
                  "type": "string"
                },
                {
                  "name": "Throughput (samples/s)",
                  "rawType": "object",
                  "type": "string"
                },
                {
                  "name": "Avg. Latency (ms/batch)",
                  "rawType": "object",
                  "type": "string"
                },
                {
                  "name": "CPU RAM Usage (MB)",
                  "rawType": "object",
                  "type": "string"
                },
                {
                  "name": "Peak GPU Memory (MB)",
                  "rawType": "object",
                  "type": "string"
                },
                {
                  "name": "Sequences",
                  "rawType": "object",
                  "type": "string"
                },
                {
                  "name": "Experiment",
                  "rawType": "object",
                  "type": "string"
                }
              ],
              "ref": "27558fb3-9a0e-470e-99ea-4d7b80b2f559",
              "rows": [
                [
                  "0",
                  "hyenadna-tiny-1k-seqlen-d256",
                  "500",
                  "20.57 s",
                  "24.31",
                  "41.01",
                  "-206.17",
                  "0.00",
                  "Original",
                  "Naive (BS=1, FP32)"
                ],
                [
                  "1",
                  "hyenadna-tiny-1k-seqlen-d256",
                  "500",
                  "20.47 s",
                  "24.42",
                  "40.83",
                  "52.56",
                  "0.00",
                  "Perturbed",
                  "Naive (BS=1, FP32)"
                ],
                [
                  "2",
                  "hyenadna-tiny-1k-seqlen-d256",
                  "500",
                  "11.01 s",
                  "45.41",
                  "687.70",
                  "252.94",
                  "0.00",
                  "Original",
                  "Batching Only (BS=32, FP32)"
                ],
                [
                  "3",
                  "hyenadna-tiny-1k-seqlen-d256",
                  "500",
                  "11.38 s",
                  "43.95",
                  "710.49",
                  "-95.02",
                  "0.00",
                  "Perturbed",
                  "Batching Only (BS=32, FP32)"
                ],
                [
                  "4",
                  "hyenadna-tiny-1k-seqlen-d256",
                  "500",
                  "10.16 s",
                  "49.24",
                  "634.19",
                  "67.09",
                  "0.00",
                  "Original",
                  "Batching with AMP (BS=32, FP16)"
                ],
                [
                  "5",
                  "hyenadna-tiny-1k-seqlen-d256",
                  "500",
                  "10.17 s",
                  "49.17",
                  "635.08",
                  "-514.53",
                  "0.00",
                  "Perturbed",
                  "Batching with AMP (BS=32, FP16)"
                ]
              ],
              "shape": {
                "columns": 9,
                "rows": 6
              }
            },
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Model name</th>\n",
              "      <th>Total samples</th>\n",
              "      <th>Total time</th>\n",
              "      <th>Throughput (samples/s)</th>\n",
              "      <th>Avg. Latency (ms/batch)</th>\n",
              "      <th>CPU RAM Usage (MB)</th>\n",
              "      <th>Peak GPU Memory (MB)</th>\n",
              "      <th>Sequences</th>\n",
              "      <th>Experiment</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>hyenadna-tiny-1k-seqlen-d256</td>\n",
              "      <td>500</td>\n",
              "      <td>20.57 s</td>\n",
              "      <td>24.31</td>\n",
              "      <td>41.01</td>\n",
              "      <td>-206.17</td>\n",
              "      <td>0.00</td>\n",
              "      <td>Original</td>\n",
              "      <td>Naive (BS=1, FP32)</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>hyenadna-tiny-1k-seqlen-d256</td>\n",
              "      <td>500</td>\n",
              "      <td>20.47 s</td>\n",
              "      <td>24.42</td>\n",
              "      <td>40.83</td>\n",
              "      <td>52.56</td>\n",
              "      <td>0.00</td>\n",
              "      <td>Perturbed</td>\n",
              "      <td>Naive (BS=1, FP32)</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>hyenadna-tiny-1k-seqlen-d256</td>\n",
              "      <td>500</td>\n",
              "      <td>11.01 s</td>\n",
              "      <td>45.41</td>\n",
              "      <td>687.70</td>\n",
              "      <td>252.94</td>\n",
              "      <td>0.00</td>\n",
              "      <td>Original</td>\n",
              "      <td>Batching Only (BS=32, FP32)</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>hyenadna-tiny-1k-seqlen-d256</td>\n",
              "      <td>500</td>\n",
              "      <td>11.38 s</td>\n",
              "      <td>43.95</td>\n",
              "      <td>710.49</td>\n",
              "      <td>-95.02</td>\n",
              "      <td>0.00</td>\n",
              "      <td>Perturbed</td>\n",
              "      <td>Batching Only (BS=32, FP32)</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>hyenadna-tiny-1k-seqlen-d256</td>\n",
              "      <td>500</td>\n",
              "      <td>10.16 s</td>\n",
              "      <td>49.24</td>\n",
              "      <td>634.19</td>\n",
              "      <td>67.09</td>\n",
              "      <td>0.00</td>\n",
              "      <td>Original</td>\n",
              "      <td>Batching with AMP (BS=32, FP16)</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>hyenadna-tiny-1k-seqlen-d256</td>\n",
              "      <td>500</td>\n",
              "      <td>10.17 s</td>\n",
              "      <td>49.17</td>\n",
              "      <td>635.08</td>\n",
              "      <td>-514.53</td>\n",
              "      <td>0.00</td>\n",
              "      <td>Perturbed</td>\n",
              "      <td>Batching with AMP (BS=32, FP16)</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                     Model name  Total samples Total time  \\\n",
              "0  hyenadna-tiny-1k-seqlen-d256            500    20.57 s   \n",
              "1  hyenadna-tiny-1k-seqlen-d256            500    20.47 s   \n",
              "2  hyenadna-tiny-1k-seqlen-d256            500    11.01 s   \n",
              "3  hyenadna-tiny-1k-seqlen-d256            500    11.38 s   \n",
              "4  hyenadna-tiny-1k-seqlen-d256            500    10.16 s   \n",
              "5  hyenadna-tiny-1k-seqlen-d256            500    10.17 s   \n",
              "\n",
              "  Throughput (samples/s) Avg. Latency (ms/batch) CPU RAM Usage (MB)  \\\n",
              "0                  24.31                   41.01            -206.17   \n",
              "1                  24.42                   40.83              52.56   \n",
              "2                  45.41                  687.70             252.94   \n",
              "3                  43.95                  710.49             -95.02   \n",
              "4                  49.24                  634.19              67.09   \n",
              "5                  49.17                  635.08            -514.53   \n",
              "\n",
              "  Peak GPU Memory (MB)  Sequences                       Experiment  \n",
              "0                 0.00   Original               Naive (BS=1, FP32)  \n",
              "1                 0.00  Perturbed               Naive (BS=1, FP32)  \n",
              "2                 0.00   Original      Batching Only (BS=32, FP32)  \n",
              "3                 0.00  Perturbed      Batching Only (BS=32, FP32)  \n",
              "4                 0.00   Original  Batching with AMP (BS=32, FP16)  \n",
              "5                 0.00  Perturbed  Batching with AMP (BS=32, FP16)  "
            ]
          },
          "execution_count": 10,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "df = pd.DataFrame(all_profiles)\n",
        "df"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "====== ON CPU (tested on 500 original sequences and 500 perturbed sequences) =====\n",
        "\n",
        "We can see that BATCHING optimization has reduced the inference time by 1.8x on both original and perturbed data when compared with Naive Inferencing\n",
        "\n",
        "Also, further applying auto mixed precision (amp) to use FP16 futher reduced the inference time by 1.08x \n",
        "\n",
        "Both the optimization combined gives inference time reduction of 2.1x\n",
        "\n",
        "\n",
        "====== ON GPU (tested on 5000 original sequences and 5000 perturbed sequences) =====\n",
        "\n",
        "We can see that BATCHING optimization has reduced the inference time by 10x on both original and perturbed data when compared with Naive Inferencing\n",
        "\n",
        "Also, further applying auto mixed precision (amp) to use FP16 futher reduced the inference time by 1.17x \n",
        "\n",
        "Both the optimization combined gives inference time reduction of 11.5x"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Validation of generated embeddings"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Comparing original embeddings generated using Naive inferencing with Batch only optimized inferencing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "--- Embedding Comparison: Naive (BS=1) vs. Batched (BS=32) ---\n",
            "Mean Absolute Error:     0.0000002405\n",
            "Average Cosine Similarity: 1.0000000000\n"
          ]
        }
      ],
      "source": [
        "compare_embeddings(naive_original_embeddings, batch_only_original_embeddings, \n",
        "                   name1=\"Naive (BS=1)\", name2=\"Batched (BS=32)\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Comparing original embeddings generated using Naive inferencing with Batch + amp optimized inferencing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "--- Embedding Comparison: Naive (BS=1) vs. Batching with AMP (BS=32, FP16) ---\n",
            "Mean Absolute Error:     0.0004565461\n",
            "Average Cosine Similarity: 0.9999992251\n"
          ]
        }
      ],
      "source": [
        "compare_embeddings(naive_original_embeddings, batch_and_amp_original_embeddings, \n",
        "                   name1=\"Naive (BS=1)\", name2=\"Batching with AMP (BS=32, FP16)\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Perturbed Embedding should be have more differences"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "--- Embedding Comparison: Naive (BS=1) vs. Batched (BS=32) ---\n",
            "Mean Absolute Error:     0.0196929798\n",
            "Average Cosine Similarity: 0.9931489229\n"
          ]
        }
      ],
      "source": [
        "compare_embeddings(naive_original_embeddings, naive_perturbed_embeddings, \n",
        "                   name1=\"Naive (BS=1)\", name2=\"Batched (BS=32)\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "This suggests that we are able to optimize inference time by multifold factor without close to no compromise on embedding generations"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": ".helical_conda_venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.13"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "13559fdf049c47628400f83d8989ac0c": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_847ccbf4f15140deb91669f39ee709f6",
            "placeholder": "​",
            "style": "IPY_MODEL_281b266c19bb4c32ac8defc4838c2d0f",
            "value": " 18/18 [00:00&lt;00:00, 119.46it/s]"
          }
        },
        "281b266c19bb4c32ac8defc4838c2d0f": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "2c472a9566e04b07b1d0964810022a22": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "2c4db2752a6b425497726e1e2317cf86": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_dcad077b721843fabd025e1af19106dd",
            "max": 18,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_a823c955f7b0411aad2a0d903a4f3b4d",
            "value": 18
          }
        },
        "347b810802ef4aeaa15de9dc9309c69b": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "39d989b0f6504e11b338dc7e7b2a90bb": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_347b810802ef4aeaa15de9dc9309c69b",
            "placeholder": "​",
            "style": "IPY_MODEL_4822183f0b504bc3b7de902c888f3143",
            "value": "Resolving data files: 100%"
          }
        },
        "4822183f0b504bc3b7de902c888f3143": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "688b915a56254c15a615691e6e84ebdf": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "715dddfbb691487192b3270d70ceb486": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_39d989b0f6504e11b338dc7e7b2a90bb",
              "IPY_MODEL_7e29a3c41cf34ac6a25837dd66559120",
              "IPY_MODEL_13559fdf049c47628400f83d8989ac0c"
            ],
            "layout": "IPY_MODEL_99291e8c19854c4494f3c0864d1ff9ee"
          }
        },
        "796af5db682b49408527e5d75450adac": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "7e29a3c41cf34ac6a25837dd66559120": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_95832f724a4345059f2e82957824fef2",
            "max": 18,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_688b915a56254c15a615691e6e84ebdf",
            "value": 18
          }
        },
        "847ccbf4f15140deb91669f39ee709f6": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "90b2f19c56954dc3a06048d79852042d": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_f2e2198a0b014e428aae57e67334239c",
              "IPY_MODEL_2c4db2752a6b425497726e1e2317cf86",
              "IPY_MODEL_b6d8ec80bbc8414b974b56d7052d85cf"
            ],
            "layout": "IPY_MODEL_796af5db682b49408527e5d75450adac"
          }
        },
        "95832f724a4345059f2e82957824fef2": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "99291e8c19854c4494f3c0864d1ff9ee": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "a823c955f7b0411aad2a0d903a4f3b4d": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "b6d8ec80bbc8414b974b56d7052d85cf": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_c7602a39c07843eaa75ba3af69d6fadb",
            "placeholder": "​",
            "style": "IPY_MODEL_c453c20a0e3f4b5d8def4c9ef9bc4431",
            "value": " 18/18 [00:00&lt;00:00, 104.71it/s]"
          }
        },
        "c453c20a0e3f4b5d8def4c9ef9bc4431": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "c7602a39c07843eaa75ba3af69d6fadb": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "c9b30f3dc6de4e2ca32ab7b40590fdd3": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "dcad077b721843fabd025e1af19106dd": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "f2e2198a0b014e428aae57e67334239c": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_2c472a9566e04b07b1d0964810022a22",
            "placeholder": "​",
            "style": "IPY_MODEL_c9b30f3dc6de4e2ca32ab7b40590fdd3",
            "value": "Resolving data files: 100%"
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
