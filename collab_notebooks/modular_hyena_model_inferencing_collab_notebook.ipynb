{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 40,
      "metadata": {
        "id": "y9-wvsGqEADW"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from torch.amp import autocast\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import psutil\n",
        "import logging\n",
        "import time\n",
        "import random\n",
        "from helical.models.hyena_dna import HyenaDNA, HyenaDNAConfig\n",
        "from datasets import load_dataset\n",
        "from tqdm import tqdm"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Task 1 : Profiling on Naive Inferencing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 65,
      "metadata": {},
      "outputs": [],
      "source": [
        "import torch\n",
        "import numpy as np\n",
        "import psutil\n",
        "import logging\n",
        "import time\n",
        "import random\n",
        "from helical.models.hyena_dna import HyenaDNA, HyenaDNAConfig\n",
        "from tqdm import tqdm"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Configurations to set before inferencing (for naive inferencing)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 66,
      "metadata": {},
      "outputs": [],
      "source": [
        "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\" # for dynamic switching between CPU and GPU based on availability \n",
        "\n",
        "MODEL_CONFIG = {\n",
        "    \"model_name\": \"hyenadna-tiny-1k-seqlen-d256\", # Model name to use\n",
        "    # \"model_name\": \"hyenadna-tiny-1k-seqlen\", # Alternate Model name to use\n",
        "    \"batch_size\": 1, # Number of sequences to process in a batch\n",
        "    \"use_amp\": False, # Use auto mixed precision for optimization\n",
        "    \"amp_dtype\": torch.float16 # will use amp_dtype only when use_amp is set to True\n",
        "}\n",
        "\n",
        "DATA_CONFIG = {\n",
        "    \"sample_size\": 50, # Change if we want a different sample size\n",
        "    \"number_of_perturbations\": 1 # Change if we want more perturbations\n",
        "}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 83,
      "metadata": {},
      "outputs": [],
      "source": [
        "# for benchmarking all the experiments\n",
        "all_profiles = []\n",
        "\n",
        "def load_hyena_model():\n",
        "    \"\"\"\n",
        "    Loads the HyenaDNA model based on settings in config.py,\n",
        "    moves it to the correct device, and returns the model object.\n",
        "    \"\"\"\n",
        "    hyena_config = HyenaDNAConfig(\n",
        "        model_name = MODEL_CONFIG[\"model_name\"], # HyenaDNA models can be used here\n",
        "    )\n",
        "    model = HyenaDNA(configurer=hyena_config)\n",
        "\n",
        "    # move model to use GPU if possible\n",
        "    if DEVICE == \"cuda\":\n",
        "        model.model.to(DEVICE)\n",
        "        \n",
        "    print(\"Model loaded successfully!\")\n",
        "    \n",
        "    return model\n",
        "  \n",
        "def compare_embeddings(embedding1, embedding2, name1=\"Embedding 1\", name2=\"Embedding 2\"):\n",
        "    \"\"\"\n",
        "    Compares two embedding tensors by calculating Mean Absolute Error and Cosine Similarity.\n",
        "\n",
        "    Args:\n",
        "        embedding1 (torch.Tensor): The first embedding tensor.\n",
        "        embedding2 (torch.Tensor): The second embedding tensor.\n",
        "        name1 (str): Name for the first embedding for printing.\n",
        "        name2 (str): Name for the second embedding for printing.\n",
        "    \"\"\"\n",
        "    # Ensure tensors are on the same device (CPU) for comparison and are float32\n",
        "    emb1_cpu = embedding1.cpu().to(torch.float32)\n",
        "    emb2_cpu = embedding2.cpu().to(torch.float32)\n",
        "\n",
        "    # 1. Calculate Mean Absolute Error (MAE)\n",
        "    mae = torch.mean(torch.abs(emb1_cpu - emb2_cpu))\n",
        "\n",
        "    # 2. Calculate Cosine Similarity\n",
        "    # We compute it for each pair of vectors (row-wise) and then average\n",
        "    cos_sim = torch.nn.functional.cosine_similarity(emb1_cpu, emb2_cpu, dim=1)\n",
        "    avg_cos_sim = torch.mean(cos_sim)\n",
        "\n",
        "    print(f\"--- Embedding Comparison: {name1} vs. {name2} ---\")\n",
        "    print(f\"Mean Absolute Error:     {mae.item():.10f}\")\n",
        "    print(f\"Average Cosine Similarity: {avg_cos_sim.item():.10f}\")\n",
        "\n",
        "  \n",
        "def get_sequences(sample_size: int):\n",
        "    \"\"\"Download the promoter_tata dataset and returns a sample of sequences\n",
        "\n",
        "    Args:\n",
        "        sample_size (int): Number of samples we want to use\n",
        "    \"\"\"\n",
        "    print(\"Downloading dataset ...\")\n",
        "    dataset = load_dataset(\n",
        "        \"InstaDeepAI/nucleotide_transformer_downstream_tasks\",\n",
        "        trust_remote_code=True\n",
        "    ).filter(lambda x: x[\"task\"] == \"promoter_tata\")\n",
        "    \n",
        "    sequences = dataset[\"train\"][\"sequence\"]\n",
        "    print(f\"Dataset is loaded, we will be taking a sample of {sample_size}\")\n",
        "    return sequences[:sample_size]\n",
        "  \n",
        "\n",
        "def add_pertubations(sequence_string: str, num_of_pertubations: int):\n",
        "  \"\"\"adds pertubations to a sequence of nucleotides\"\"\"\n",
        "  nucleotides = [\"A\", \"G\", \"T\", \"C\"]\n",
        "  length = len(sequence_string)\n",
        "  seq_list = list(sequence_string)\n",
        "\n",
        "  for _ in range(num_of_pertubations):\n",
        "    random_idx = np.random.randint(0, length - 1)\n",
        "\n",
        "    original_nucleotide = seq_list[random_idx]\n",
        "    possible_pertubations = [n for n in nucleotides if n != original_nucleotide]\n",
        "    new_nucleotide = random.choice(possible_pertubations)\n",
        "\n",
        "    # apply the pertubation to mutate\n",
        "    seq_list[random_idx] = new_nucleotide\n",
        "\n",
        "  # return perturbed sequence\n",
        "  return \"\".join(seq_list)\n",
        "\n",
        "def log_inference_profile(\n",
        "  total_time: float,\n",
        "  latencies: list,\n",
        "  num_samples: int,\n",
        "  start_rss_mb: float\n",
        "):\n",
        "  \"\"\"\n",
        "  Calculates and logs inference related metrics of the run\n",
        "  \"\"\"\n",
        "  avg_latency = np.mean(latencies) * 1000 # to convert in ms\n",
        "  throughput = DATA_CONFIG[\"sample_size\"]/total_time\n",
        "  end_rss_mb = psutil.Process().memory_info().rss / (1024 * 1024)\n",
        "  \n",
        "  # get GPU memory if running on GPU\n",
        "  peak_gpu_mb = 0\n",
        "  if DEVICE == \"cuda\" and torch.cuda.is_available():\n",
        "    peak_gpu_mb = torch.cuda.max_memory_allocated() / (1024 * 1024)\n",
        "    \n",
        "  log_line = f\"\"\"\n",
        "------------ Inference Profile ------------\n",
        "Device:                 {DEVICE.upper()}\n",
        "Model name:             {MODEL_CONFIG[\"model_name\"]}\n",
        "amp enabled:            {MODEL_CONFIG[\"use_amp\"]}\n",
        "amp dtype:              {MODEL_CONFIG[\"amp_dtype\"] if MODEL_CONFIG[\"use_amp\"] else \"torch.float32 (default)\"}\n",
        "Total Samples:          {num_samples}\n",
        "Batch Size:             {MODEL_CONFIG[\"batch_size\"]}\n",
        "---\n",
        "Total Time:             {total_time:.2f} s\n",
        "Throughput:             {throughput:.2f} samples/s\n",
        "Avg. Latency / Batch:   {avg_latency:.2f} ms\n",
        "---\n",
        "CPU RAM Usage:          {end_rss_mb - start_rss_mb:.2f} MB\n",
        "Peak GPU Memory:        {peak_gpu_mb:.2f} MB\n",
        "---------------------------------------------------\n",
        "\"\"\"\n",
        "  # Create a dictionary of the results\n",
        "  profile_results = {\n",
        "      \"Model name\":  MODEL_CONFIG[\"model_name\"],\n",
        "      \"Total samples\": num_samples,\n",
        "      \"Total time\": f\"{total_time:.2f} s\",\n",
        "      \"Throughput (samples/s)\": f\"{throughput:.2f}\",\n",
        "      \"Avg. Latency (ms/batch)\": f\"{avg_latency:.2f}\",\n",
        "      \"CPU RAM Usage (MB)\": f\"{end_rss_mb - start_rss_mb:.2f}\",\n",
        "      \"Peak GPU Memory (MB)\": f\"{peak_gpu_mb:.2f}\"\n",
        "  }\n",
        "  \n",
        "  # Reset peak memory stats for the next run if needed\n",
        "  if DEVICE == \"cuda\" and torch.cuda.is_available():\n",
        "      torch.cuda.reset_peak_memory_stats()\n",
        "      \n",
        "  return log_line, profile_results\n",
        "\n",
        "\n",
        "def run_hyena_inferencing(model, sequences_to_process: list):\n",
        "    \"\"\"runs inferencing on sequences\"\"\"\n",
        "    pertubation_embeddings = []\n",
        "    latencies = []\n",
        "\n",
        "    # Inference on pertubations\n",
        "    overall_start = time.time()\n",
        "    start_rss = psutil.Process().memory_info().rss / (1024 * 1024)\n",
        "    BATCH_SIZE = MODEL_CONFIG[\"batch_size\"]\n",
        "    overall_start = time.time()\n",
        "\n",
        "    for i in range(0, DATA_CONFIG[\"sample_size\"], BATCH_SIZE):\n",
        "        t_loop_in = time.time()\n",
        "        raw_tokens = model.process_data(sequences_to_process[i:i + BATCH_SIZE])\n",
        "        input_ids_tensor = torch.tensor(raw_tokens[\"input_ids\"]).to(DEVICE)\n",
        "\n",
        "        with torch.no_grad():\n",
        "            with autocast(DEVICE, enabled=MODEL_CONFIG[\"use_amp\"], dtype=MODEL_CONFIG[\"amp_dtype\"]):\n",
        "                outputs = model.model(input_ids=input_ids_tensor)\n",
        "                embeddings = outputs\n",
        "\n",
        "        t_loop_out = time.time()\n",
        "        latencies.append(t_loop_out - t_loop_in)\n",
        "        \n",
        "        if isinstance(embeddings, torch.Tensor):\n",
        "            pertubation_embeddings.append(embeddings)\n",
        "\n",
        "    total_time = time.time() - overall_start\n",
        "\n",
        "    # Call the logging function from utils\n",
        "    run_profile, run_profile_dict = log_inference_profile(\n",
        "        total_time=total_time,\n",
        "        latencies=latencies,\n",
        "        num_samples=len(sequences_to_process),\n",
        "        start_rss_mb=start_rss\n",
        "    )\n",
        "    \n",
        "    print(run_profile)\n",
        "\n",
        "    return torch.cat(pertubation_embeddings, dim=0), run_profile_dict"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Profiling the inferencing on perturbated sequences (NAIVE)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 84,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Downloading dataset ...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2026-01-14 13:26:27,107 - INFO:helical.models.hyena_dna.pretrained_model:Loaded pretrained weights ok!\n",
            "2026-01-14 13:26:27,110 - INFO:helical.models.hyena_dna.model:Model finished initializing.\n",
            "2026-01-14 13:26:27,111 - INFO:helical.models.hyena_dna.model:'hyenadna-tiny-1k-seqlen-d256' model is in 'eval' mode, on device 'cpu'.\n",
            "2026-01-14 13:26:27,112 - INFO:helical.models.hyena_dna.model:Processing data for HyenaDNA.\n",
            "2026-01-14 13:26:27,123 - INFO:helical.models.hyena_dna.model:Succesfully prepared the HyenaDNA Dataset.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Dataset is loaded, we will be taking a sample of 50\n",
            "Loading Hyena model...\n",
            "Model loaded successfully!\n",
            "Starting inference run on original sequences...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2026-01-14 13:26:27,885 - INFO:helical.models.hyena_dna.model:Processing data for HyenaDNA.\n",
            "2026-01-14 13:26:27,891 - INFO:helical.models.hyena_dna.model:Succesfully prepared the HyenaDNA Dataset.\n",
            "2026-01-14 13:26:28,433 - INFO:helical.models.hyena_dna.model:Processing data for HyenaDNA.\n",
            "2026-01-14 13:26:28,442 - INFO:helical.models.hyena_dna.model:Succesfully prepared the HyenaDNA Dataset.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "------------ Inference Profile ------------\n",
            "Device:                 CPU\n",
            "Model name:             hyenadna-tiny-1k-seqlen-d256\n",
            "amp enabled:            False\n",
            "amp dtype:              torch.float32 (default)\n",
            "Total Samples:          50\n",
            "Batch Size:             32\n",
            "---\n",
            "Total Time:             1.32 s\n",
            "Throughput:             37.93 samples/s\n",
            "Avg. Latency / Batch:   659.09 ms\n",
            "---\n",
            "CPU RAM Usage:          310.94 MB\n",
            "Peak GPU Memory:        0.00 MB\n",
            "---------------------------------------------------\n",
            "\n",
            "Starting inference run on perturbed sequences...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2026-01-14 13:26:29,132 - INFO:helical.models.hyena_dna.model:Processing data for HyenaDNA.\n",
            "2026-01-14 13:26:29,138 - INFO:helical.models.hyena_dna.model:Succesfully prepared the HyenaDNA Dataset.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "------------ Inference Profile ------------\n",
            "Device:                 CPU\n",
            "Model name:             hyenadna-tiny-1k-seqlen-d256\n",
            "amp enabled:            False\n",
            "amp dtype:              torch.float32 (default)\n",
            "Total Samples:          50\n",
            "Batch Size:             32\n",
            "---\n",
            "Total Time:             1.24 s\n",
            "Throughput:             40.37 samples/s\n",
            "Avg. Latency / Batch:   619.26 ms\n",
            "---\n",
            "CPU RAM Usage:          39.50 MB\n",
            "Peak GPU Memory:        0.00 MB\n",
            "---------------------------------------------------\n",
            "\n",
            "Successfully generated original embeddings of shape: torch.Size([50, 302, 256])\n",
            "Successfully generated perturbed embeddings of shape: torch.Size([50, 302, 256])\n"
          ]
        }
      ],
      "source": [
        "time_signature = time.strftime('%Y%m%d-%H%M%S')\n",
        "\n",
        "sequences = get_sequences(DATA_CONFIG[\"sample_size\"])\n",
        "\n",
        "perturbed_sequences = []\n",
        "\n",
        "for sequence in sequences:\n",
        "    perturbed_sequences.append(add_pertubations(sequence, num_of_pertubations=1))\n",
        "\n",
        "perturbed_sequences = [add_pertubations(seq, num_of_pertubations=DATA_CONFIG[\"number_of_perturbations\"]) for seq in sequences]\n",
        "\n",
        "print(\"Loading Hyena model...\")\n",
        "model = load_hyena_model()\n",
        "\n",
        "print(\"Starting inference run on original sequences...\")\n",
        "naive_original_embeddings, naive_original_profile = run_hyena_inferencing(model, sequences)\n",
        "naive_original_profile[\"Sequences\"] = \"Original\"\n",
        "naive_original_profile[\"Experiment\"] = \"Naive (BS=1, FP32)\"\n",
        "all_profiles.append(naive_original_profile)\n",
        "\n",
        "print(\"Starting inference run on perturbed sequences...\")\n",
        "naive_perturbed_embeddings, naive_perturbed_profile = run_hyena_inferencing(model, perturbed_sequences)\n",
        "naive_perturbed_profile[\"Sequences\"] = \"Perturbed\"\n",
        "naive_perturbed_profile[\"Experiment\"] = \"Naive (BS=1, FP32)\"\n",
        "all_profiles.append(naive_perturbed_profile)\n",
        "\n",
        "\n",
        "print(f\"Successfully generated original embeddings of shape: {naive_original_embeddings.shape}\")\n",
        "print(f\"Successfully generated perturbed embeddings of shape: {perturbed_embeddings.shape}\")\n",
        "\n",
        "original_embedding_array = np.stack(naive_original_embeddings)\n",
        "perturbed_embedding_array = np.stack(naive_perturbed_embeddings)\n",
        "\n",
        "np.save(f\"original_embedding_{time_signature}.npy\", original_embedding_array)\n",
        "np.save(f\"perturbed_embedding_{time_signature}.npy\", original_embedding_array)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Task 2: Scale ISP and Optimizations"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Optimization 1 : Batching (Batch Size = 32)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Configurations to set before inferencing (for batching optimization)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 85,
      "metadata": {},
      "outputs": [],
      "source": [
        "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\" # for dynamic switching between CPU and GPU based on availability \n",
        "\n",
        "MODEL_CONFIG = {\n",
        "    \"model_name\": \"hyenadna-tiny-1k-seqlen-d256\", # Model name to use\n",
        "    # \"model_name\": \"hyenadna-tiny-1k-seqlen\", # Alternate Model name to use\n",
        "    \"batch_size\": 32, # Number of sequences to process in a batch\n",
        "    \"use_amp\": False, # Use auto mixed precision for optimization\n",
        "    \"amp_dtype\": torch.float16 # will use amp_dtype only when use_amp is set to True\n",
        "}\n",
        "\n",
        "DATA_CONFIG = {\n",
        "    \"sample_size\": 50, # Change if we want a different sample size\n",
        "    \"number_of_perturbations\": 1 # Change if we want more perturbations\n",
        "}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 86,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Downloading dataset ...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2026-01-14 13:26:30,564 - INFO:helical.models.hyena_dna.pretrained_model:Loaded pretrained weights ok!\n",
            "2026-01-14 13:26:30,565 - INFO:helical.models.hyena_dna.model:Model finished initializing.\n",
            "2026-01-14 13:26:30,565 - INFO:helical.models.hyena_dna.model:'hyenadna-tiny-1k-seqlen-d256' model is in 'eval' mode, on device 'cpu'.\n",
            "2026-01-14 13:26:30,567 - INFO:helical.models.hyena_dna.model:Processing data for HyenaDNA.\n",
            "2026-01-14 13:26:30,576 - INFO:helical.models.hyena_dna.model:Succesfully prepared the HyenaDNA Dataset.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Dataset is loaded, we will be taking a sample of 50\n",
            "Loading Hyena model...\n",
            "Model loaded successfully!\n",
            "Starting inference run on original sequences...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2026-01-14 13:26:31,269 - INFO:helical.models.hyena_dna.model:Processing data for HyenaDNA.\n",
            "2026-01-14 13:26:31,275 - INFO:helical.models.hyena_dna.model:Succesfully prepared the HyenaDNA Dataset.\n",
            "2026-01-14 13:26:31,807 - INFO:helical.models.hyena_dna.model:Processing data for HyenaDNA.\n",
            "2026-01-14 13:26:31,816 - INFO:helical.models.hyena_dna.model:Succesfully prepared the HyenaDNA Dataset.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "------------ Inference Profile ------------\n",
            "Device:                 CPU\n",
            "Model name:             hyenadna-tiny-1k-seqlen-d256\n",
            "amp enabled:            False\n",
            "amp dtype:              torch.float32 (default)\n",
            "Total Samples:          50\n",
            "Batch Size:             32\n",
            "---\n",
            "Total Time:             1.24 s\n",
            "Throughput:             40.40 samples/s\n",
            "Avg. Latency / Batch:   618.84 ms\n",
            "---\n",
            "CPU RAM Usage:          54.94 MB\n",
            "Peak GPU Memory:        0.00 MB\n",
            "---------------------------------------------------\n",
            "\n",
            "Starting inference run on perturbed sequences...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2026-01-14 13:26:32,490 - INFO:helical.models.hyena_dna.model:Processing data for HyenaDNA.\n",
            "2026-01-14 13:26:32,496 - INFO:helical.models.hyena_dna.model:Succesfully prepared the HyenaDNA Dataset.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "------------ Inference Profile ------------\n",
            "Device:                 CPU\n",
            "Model name:             hyenadna-tiny-1k-seqlen-d256\n",
            "amp enabled:            False\n",
            "amp dtype:              torch.float32 (default)\n",
            "Total Samples:          50\n",
            "Batch Size:             32\n",
            "---\n",
            "Total Time:             1.22 s\n",
            "Throughput:             41.05 samples/s\n",
            "Avg. Latency / Batch:   608.98 ms\n",
            "---\n",
            "CPU RAM Usage:          48.95 MB\n",
            "Peak GPU Memory:        0.00 MB\n",
            "---------------------------------------------------\n",
            "\n",
            "Successfully generated original embeddings of shape: torch.Size([50, 302, 256])\n",
            "Successfully generated perturbed embeddings of shape: torch.Size([50, 302, 256])\n"
          ]
        }
      ],
      "source": [
        "time_signature = time.strftime('%Y%m%d-%H%M%S')\n",
        "\n",
        "sequences = get_sequences(DATA_CONFIG[\"sample_size\"])\n",
        "\n",
        "perturbed_sequences = []\n",
        "\n",
        "for sequence in sequences:\n",
        "    perturbed_sequences.append(add_pertubations(sequence, num_of_pertubations=1))\n",
        "\n",
        "perturbed_sequences = [add_pertubations(seq, num_of_pertubations=DATA_CONFIG[\"number_of_perturbations\"]) for seq in sequences]\n",
        "\n",
        "print(\"Loading Hyena model...\")\n",
        "model = load_hyena_model()\n",
        "\n",
        "print(\"Starting inference run on original sequences...\")\n",
        "batch_only_original_embeddings, optimization_one_original_profile = run_hyena_inferencing(model, sequences)\n",
        "optimization_one_original_profile[\"Sequences\"] = \"Original\"\n",
        "optimization_one_original_profile[\"Experiment\"] = \"Batching Only (BS=32, FP32)\"\n",
        "all_profiles.append(optimization_one_original_profile)\n",
        "\n",
        "print(\"Starting inference run on perturbed sequences...\")\n",
        "batch_only_perturbed_embeddings, optimization_one_perturbed_profile = run_hyena_inferencing(model, perturbed_sequences)\n",
        "optimization_one_perturbed_profile[\"Sequences\"] = \"Perturbed\"\n",
        "optimization_one_perturbed_profile[\"Experiment\"] = \"Batching Only (BS=32, FP32)\"\n",
        "all_profiles.append(optimization_one_perturbed_profile)\n",
        "\n",
        "print(f\"Successfully generated original embeddings of shape: {batch_only_original_embeddings.shape}\")\n",
        "print(f\"Successfully generated perturbed embeddings of shape: {perturbed_embeddings.shape}\")\n",
        "\n",
        "original_embedding_array = np.stack(batch_only_original_embeddings)\n",
        "perturbed_embedding_array = np.stack(batch_only_perturbed_embeddings)\n",
        "\n",
        "np.save(f\"original_embedding_{time_signature}.npy\", original_embedding_array)\n",
        "np.save(f\"perturbed_embedding_{time_signature}.npy\", original_embedding_array)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Optimization 2 : Mixed Precision (Batch Size = 32)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Configurations to set before inferencing (for batching optimization)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 87,
      "metadata": {},
      "outputs": [],
      "source": [
        "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\" # for dynamic switching between CPU and GPU based on availability \n",
        "\n",
        "MODEL_CONFIG = {\n",
        "    \"model_name\": \"hyenadna-tiny-1k-seqlen-d256\", # Model name to use\n",
        "    # \"model_name\": \"hyenadna-tiny-1k-seqlen\", # Alternate Model name to use\n",
        "    \"batch_size\": 32, # Number of sequences to process in a batch\n",
        "    \"use_amp\": True, # Use auto mixed precision for optimization\n",
        "    \"amp_dtype\": torch.float16 # will use amp_dtype only when use_amp is set to True\n",
        "}\n",
        "\n",
        "DATA_CONFIG = {\n",
        "    \"sample_size\": 50, # Change if we want a different sample size\n",
        "    \"number_of_perturbations\": 1 # Change if we want more perturbations\n",
        "}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 88,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Downloading dataset ...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2026-01-14 13:26:34,043 - INFO:helical.models.hyena_dna.pretrained_model:Loaded pretrained weights ok!\n",
            "2026-01-14 13:26:34,045 - INFO:helical.models.hyena_dna.model:Model finished initializing.\n",
            "2026-01-14 13:26:34,045 - INFO:helical.models.hyena_dna.model:'hyenadna-tiny-1k-seqlen-d256' model is in 'eval' mode, on device 'cpu'.\n",
            "2026-01-14 13:26:34,048 - INFO:helical.models.hyena_dna.model:Processing data for HyenaDNA.\n",
            "2026-01-14 13:26:34,057 - INFO:helical.models.hyena_dna.model:Succesfully prepared the HyenaDNA Dataset.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Dataset is loaded, we will be taking a sample of 50\n",
            "Loading Hyena model...\n",
            "Model loaded successfully!\n",
            "Starting inference run on original sequences...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2026-01-14 13:26:34,651 - INFO:helical.models.hyena_dna.model:Processing data for HyenaDNA.\n",
            "2026-01-14 13:26:34,658 - INFO:helical.models.hyena_dna.model:Succesfully prepared the HyenaDNA Dataset.\n",
            "2026-01-14 13:26:35,002 - INFO:helical.models.hyena_dna.model:Processing data for HyenaDNA.\n",
            "2026-01-14 13:26:35,011 - INFO:helical.models.hyena_dna.model:Succesfully prepared the HyenaDNA Dataset.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "------------ Inference Profile ------------\n",
            "Device:                 CPU\n",
            "Model name:             hyenadna-tiny-1k-seqlen-d256\n",
            "amp enabled:            True\n",
            "amp dtype:              torch.float16\n",
            "Total Samples:          50\n",
            "Batch Size:             32\n",
            "---\n",
            "Total Time:             0.95 s\n",
            "Throughput:             52.46 samples/s\n",
            "Avg. Latency / Batch:   476.59 ms\n",
            "---\n",
            "CPU RAM Usage:          -104.30 MB\n",
            "Peak GPU Memory:        0.00 MB\n",
            "---------------------------------------------------\n",
            "\n",
            "Starting inference run on perturbed sequences...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2026-01-14 13:26:35,589 - INFO:helical.models.hyena_dna.model:Processing data for HyenaDNA.\n",
            "2026-01-14 13:26:35,595 - INFO:helical.models.hyena_dna.model:Succesfully prepared the HyenaDNA Dataset.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "------------ Inference Profile ------------\n",
            "Device:                 CPU\n",
            "Model name:             hyenadna-tiny-1k-seqlen-d256\n",
            "amp enabled:            True\n",
            "amp dtype:              torch.float16\n",
            "Total Samples:          50\n",
            "Batch Size:             32\n",
            "---\n",
            "Total Time:             0.93 s\n",
            "Throughput:             53.62 samples/s\n",
            "Avg. Latency / Batch:   466.24 ms\n",
            "---\n",
            "CPU RAM Usage:          57.64 MB\n",
            "Peak GPU Memory:        0.00 MB\n",
            "---------------------------------------------------\n",
            "\n",
            "Successfully generated original embeddings of shape: torch.Size([50, 302, 256])\n",
            "Successfully generated perturbed embeddings of shape: torch.Size([50, 302, 256])\n"
          ]
        }
      ],
      "source": [
        "time_signature = time.strftime('%Y%m%d-%H%M%S')\n",
        "\n",
        "sequences = get_sequences(DATA_CONFIG[\"sample_size\"])\n",
        "\n",
        "perturbed_sequences = []\n",
        "\n",
        "for sequence in sequences:\n",
        "    perturbed_sequences.append(add_pertubations(sequence, num_of_pertubations=1))\n",
        "\n",
        "perturbed_sequences = [add_pertubations(seq, num_of_pertubations=DATA_CONFIG[\"number_of_perturbations\"]) for seq in sequences]\n",
        "\n",
        "print(\"Loading Hyena model...\")\n",
        "model = load_hyena_model()\n",
        "\n",
        "print(\"Starting inference run on original sequences...\")\n",
        "batch_and_amp_original_embeddings, optimization_two_original_profile = run_hyena_inferencing(model, sequences)\n",
        "optimization_two_original_profile[\"Sequences\"] = \"Original\"\n",
        "optimization_two_original_profile[\"Experiment\"] = \"Batching with AMP (BS=32, FP16)\"\n",
        "all_profiles.append(optimization_two_original_profile)\n",
        "\n",
        "print(\"Starting inference run on perturbed sequences...\")\n",
        "batch_and_amp_perturbed_embeddings, optimization_two_perturbed_profile = run_hyena_inferencing(model, perturbed_sequences)\n",
        "optimization_two_perturbed_profile[\"Sequences\"] = \"Perturbed\"\n",
        "optimization_two_perturbed_profile[\"Experiment\"] = \"Batching with AMP (BS=32, FP16)\"\n",
        "all_profiles.append(optimization_two_perturbed_profile)\n",
        "\n",
        "print(f\"Successfully generated original embeddings of shape: {batch_and_amp_original_embeddings.shape}\")\n",
        "print(f\"Successfully generated perturbed embeddings of shape: {batch_and_amp_perturbed_embeddings.shape}\")\n",
        "\n",
        "original_embedding_array = np.stack(batch_and_amp_original_embeddings)\n",
        "perturbed_embedding_array = np.stack(batch_and_amp_perturbed_embeddings)\n",
        "\n",
        "np.save(f\"original_embedding_{time_signature}.npy\", original_embedding_array)\n",
        "np.save(f\"perturbed_embedding_{time_signature}.npy\", original_embedding_array)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Summarizing Results"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 89,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "application/vnd.microsoft.datawrangler.viewer.v0+json": {
              "columns": [
                {
                  "name": "index",
                  "rawType": "int64",
                  "type": "integer"
                },
                {
                  "name": "Model name",
                  "rawType": "object",
                  "type": "string"
                },
                {
                  "name": "Total samples",
                  "rawType": "int64",
                  "type": "integer"
                },
                {
                  "name": "Total time",
                  "rawType": "object",
                  "type": "string"
                },
                {
                  "name": "Throughput (samples/s)",
                  "rawType": "object",
                  "type": "string"
                },
                {
                  "name": "Avg. Latency (ms/batch)",
                  "rawType": "object",
                  "type": "string"
                },
                {
                  "name": "CPU RAM Usage (MB)",
                  "rawType": "object",
                  "type": "string"
                },
                {
                  "name": "Peak GPU Memory (MB)",
                  "rawType": "object",
                  "type": "string"
                },
                {
                  "name": "Sequences",
                  "rawType": "object",
                  "type": "string"
                },
                {
                  "name": "Experiment",
                  "rawType": "object",
                  "type": "string"
                }
              ],
              "ref": "337b0441-e3ad-49e2-a675-4ed82e9fb881",
              "rows": [
                [
                  "0",
                  "hyenadna-tiny-1k-seqlen-d256",
                  "50",
                  "1.32 s",
                  "37.93",
                  "659.09",
                  "310.94",
                  "0.00",
                  "Original",
                  "Naive (BS=1, FP32)"
                ],
                [
                  "1",
                  "hyenadna-tiny-1k-seqlen-d256",
                  "50",
                  "1.24 s",
                  "40.37",
                  "619.26",
                  "39.50",
                  "0.00",
                  "Perturbed",
                  "Naive (BS=1, FP32)"
                ],
                [
                  "2",
                  "hyenadna-tiny-1k-seqlen-d256",
                  "50",
                  "1.24 s",
                  "40.40",
                  "618.84",
                  "54.94",
                  "0.00",
                  "Original",
                  "Batching Only (BS=32, FP32)"
                ],
                [
                  "3",
                  "hyenadna-tiny-1k-seqlen-d256",
                  "50",
                  "1.22 s",
                  "41.05",
                  "608.98",
                  "48.95",
                  "0.00",
                  "Perturbed",
                  "Batching Only (BS=32, FP32)"
                ],
                [
                  "4",
                  "hyenadna-tiny-1k-seqlen-d256",
                  "50",
                  "0.95 s",
                  "52.46",
                  "476.59",
                  "-104.30",
                  "0.00",
                  "Original",
                  "Batching with AMP (BS=32, FP16)"
                ],
                [
                  "5",
                  "hyenadna-tiny-1k-seqlen-d256",
                  "50",
                  "0.93 s",
                  "53.62",
                  "466.24",
                  "57.64",
                  "0.00",
                  "Perturbed",
                  "Batching with AMP (BS=32, FP16)"
                ]
              ],
              "shape": {
                "columns": 9,
                "rows": 6
              }
            },
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Model name</th>\n",
              "      <th>Total samples</th>\n",
              "      <th>Total time</th>\n",
              "      <th>Throughput (samples/s)</th>\n",
              "      <th>Avg. Latency (ms/batch)</th>\n",
              "      <th>CPU RAM Usage (MB)</th>\n",
              "      <th>Peak GPU Memory (MB)</th>\n",
              "      <th>Sequences</th>\n",
              "      <th>Experiment</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>hyenadna-tiny-1k-seqlen-d256</td>\n",
              "      <td>50</td>\n",
              "      <td>1.32 s</td>\n",
              "      <td>37.93</td>\n",
              "      <td>659.09</td>\n",
              "      <td>310.94</td>\n",
              "      <td>0.00</td>\n",
              "      <td>Original</td>\n",
              "      <td>Naive (BS=1, FP32)</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>hyenadna-tiny-1k-seqlen-d256</td>\n",
              "      <td>50</td>\n",
              "      <td>1.24 s</td>\n",
              "      <td>40.37</td>\n",
              "      <td>619.26</td>\n",
              "      <td>39.50</td>\n",
              "      <td>0.00</td>\n",
              "      <td>Perturbed</td>\n",
              "      <td>Naive (BS=1, FP32)</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>hyenadna-tiny-1k-seqlen-d256</td>\n",
              "      <td>50</td>\n",
              "      <td>1.24 s</td>\n",
              "      <td>40.40</td>\n",
              "      <td>618.84</td>\n",
              "      <td>54.94</td>\n",
              "      <td>0.00</td>\n",
              "      <td>Original</td>\n",
              "      <td>Batching Only (BS=32, FP32)</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>hyenadna-tiny-1k-seqlen-d256</td>\n",
              "      <td>50</td>\n",
              "      <td>1.22 s</td>\n",
              "      <td>41.05</td>\n",
              "      <td>608.98</td>\n",
              "      <td>48.95</td>\n",
              "      <td>0.00</td>\n",
              "      <td>Perturbed</td>\n",
              "      <td>Batching Only (BS=32, FP32)</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>hyenadna-tiny-1k-seqlen-d256</td>\n",
              "      <td>50</td>\n",
              "      <td>0.95 s</td>\n",
              "      <td>52.46</td>\n",
              "      <td>476.59</td>\n",
              "      <td>-104.30</td>\n",
              "      <td>0.00</td>\n",
              "      <td>Original</td>\n",
              "      <td>Batching with AMP (BS=32, FP16)</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>hyenadna-tiny-1k-seqlen-d256</td>\n",
              "      <td>50</td>\n",
              "      <td>0.93 s</td>\n",
              "      <td>53.62</td>\n",
              "      <td>466.24</td>\n",
              "      <td>57.64</td>\n",
              "      <td>0.00</td>\n",
              "      <td>Perturbed</td>\n",
              "      <td>Batching with AMP (BS=32, FP16)</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                     Model name  Total samples Total time  \\\n",
              "0  hyenadna-tiny-1k-seqlen-d256             50     1.32 s   \n",
              "1  hyenadna-tiny-1k-seqlen-d256             50     1.24 s   \n",
              "2  hyenadna-tiny-1k-seqlen-d256             50     1.24 s   \n",
              "3  hyenadna-tiny-1k-seqlen-d256             50     1.22 s   \n",
              "4  hyenadna-tiny-1k-seqlen-d256             50     0.95 s   \n",
              "5  hyenadna-tiny-1k-seqlen-d256             50     0.93 s   \n",
              "\n",
              "  Throughput (samples/s) Avg. Latency (ms/batch) CPU RAM Usage (MB)  \\\n",
              "0                  37.93                  659.09             310.94   \n",
              "1                  40.37                  619.26              39.50   \n",
              "2                  40.40                  618.84              54.94   \n",
              "3                  41.05                  608.98              48.95   \n",
              "4                  52.46                  476.59            -104.30   \n",
              "5                  53.62                  466.24              57.64   \n",
              "\n",
              "  Peak GPU Memory (MB)  Sequences                       Experiment  \n",
              "0                 0.00   Original               Naive (BS=1, FP32)  \n",
              "1                 0.00  Perturbed               Naive (BS=1, FP32)  \n",
              "2                 0.00   Original      Batching Only (BS=32, FP32)  \n",
              "3                 0.00  Perturbed      Batching Only (BS=32, FP32)  \n",
              "4                 0.00   Original  Batching with AMP (BS=32, FP16)  \n",
              "5                 0.00  Perturbed  Batching with AMP (BS=32, FP16)  "
            ]
          },
          "execution_count": 89,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "df = pd.DataFrame(all_profiles)\n",
        "df"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We can see that BATCHING optimization has reduced the inference time by 1.8x on both original and perturbed data when compared with Naive Inferencing\n",
        "\n",
        "Also, further applying auto mixed precision (amp) to use FP16 futher reduced the inference time by 1.3x \n",
        "\n",
        "Both the optimization combined gives inference time reduction of 2.3x"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Validation of generated embeddings"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Comparing original embeddings generated using Naive inferencing with Batch only optimized inferencing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 90,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "--- Embedding Comparison: Naive (BS=1) vs. Batched (BS=32) ---\n",
            "Mean Absolute Error:     0.0000000000\n",
            "Average Cosine Similarity: 1.0000000000\n"
          ]
        }
      ],
      "source": [
        "compare_embeddings(naive_original_embeddings, batch_only_original_embeddings, \n",
        "                   name1=\"Naive (BS=1)\", name2=\"Batched (BS=32)\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Comparing original embeddings generated using Naive inferencing with Batch + amp optimized inferencing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 91,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "--- Embedding Comparison: Naive (BS=1) vs. Batching with AMP (BS=32, FP16) ---\n",
            "Mean Absolute Error:     0.0004380128\n",
            "Average Cosine Similarity: 0.9999994040\n"
          ]
        }
      ],
      "source": [
        "compare_embeddings(naive_original_embeddings, batch_and_amp_original_embeddings, \n",
        "                   name1=\"Naive (BS=1)\", name2=\"Batching with AMP (BS=32, FP16)\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Perturbed Embedding should be have more differences"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 92,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "--- Embedding Comparison: Naive (BS=1) vs. Batched (BS=32) ---\n",
            "Mean Absolute Error:     0.0186630860\n",
            "Average Cosine Similarity: 0.9931645989\n"
          ]
        }
      ],
      "source": [
        "compare_embeddings(naive_original_embeddings, naive_perturbed_embeddings, \n",
        "                   name1=\"Naive (BS=1)\", name2=\"Batched (BS=32)\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "This suggests that we are able to optimize inference time by multifold factor without close to no compromise on embedding generations"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": ".helical_conda_venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.13"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "13559fdf049c47628400f83d8989ac0c": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_847ccbf4f15140deb91669f39ee709f6",
            "placeholder": "​",
            "style": "IPY_MODEL_281b266c19bb4c32ac8defc4838c2d0f",
            "value": " 18/18 [00:00&lt;00:00, 119.46it/s]"
          }
        },
        "281b266c19bb4c32ac8defc4838c2d0f": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "2c472a9566e04b07b1d0964810022a22": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "2c4db2752a6b425497726e1e2317cf86": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_dcad077b721843fabd025e1af19106dd",
            "max": 18,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_a823c955f7b0411aad2a0d903a4f3b4d",
            "value": 18
          }
        },
        "347b810802ef4aeaa15de9dc9309c69b": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "39d989b0f6504e11b338dc7e7b2a90bb": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_347b810802ef4aeaa15de9dc9309c69b",
            "placeholder": "​",
            "style": "IPY_MODEL_4822183f0b504bc3b7de902c888f3143",
            "value": "Resolving data files: 100%"
          }
        },
        "4822183f0b504bc3b7de902c888f3143": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "688b915a56254c15a615691e6e84ebdf": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "715dddfbb691487192b3270d70ceb486": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_39d989b0f6504e11b338dc7e7b2a90bb",
              "IPY_MODEL_7e29a3c41cf34ac6a25837dd66559120",
              "IPY_MODEL_13559fdf049c47628400f83d8989ac0c"
            ],
            "layout": "IPY_MODEL_99291e8c19854c4494f3c0864d1ff9ee"
          }
        },
        "796af5db682b49408527e5d75450adac": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "7e29a3c41cf34ac6a25837dd66559120": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_95832f724a4345059f2e82957824fef2",
            "max": 18,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_688b915a56254c15a615691e6e84ebdf",
            "value": 18
          }
        },
        "847ccbf4f15140deb91669f39ee709f6": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "90b2f19c56954dc3a06048d79852042d": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_f2e2198a0b014e428aae57e67334239c",
              "IPY_MODEL_2c4db2752a6b425497726e1e2317cf86",
              "IPY_MODEL_b6d8ec80bbc8414b974b56d7052d85cf"
            ],
            "layout": "IPY_MODEL_796af5db682b49408527e5d75450adac"
          }
        },
        "95832f724a4345059f2e82957824fef2": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "99291e8c19854c4494f3c0864d1ff9ee": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "a823c955f7b0411aad2a0d903a4f3b4d": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "b6d8ec80bbc8414b974b56d7052d85cf": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_c7602a39c07843eaa75ba3af69d6fadb",
            "placeholder": "​",
            "style": "IPY_MODEL_c453c20a0e3f4b5d8def4c9ef9bc4431",
            "value": " 18/18 [00:00&lt;00:00, 104.71it/s]"
          }
        },
        "c453c20a0e3f4b5d8def4c9ef9bc4431": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "c7602a39c07843eaa75ba3af69d6fadb": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "c9b30f3dc6de4e2ca32ab7b40590fdd3": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "dcad077b721843fabd025e1af19106dd": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "f2e2198a0b014e428aae57e67334239c": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_2c472a9566e04b07b1d0964810022a22",
            "placeholder": "​",
            "style": "IPY_MODEL_c9b30f3dc6de4e2ca32ab7b40590fdd3",
            "value": "Resolving data files: 100%"
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
