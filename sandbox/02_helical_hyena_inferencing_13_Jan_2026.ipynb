{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "y9-wvsGqEADW"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2026-01-13 19:52:43,171 - WARNING:py.warnings:/opt/homebrew/Caskroom/miniforge/base/envs/.helical_conda_venv/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
            "  from .autonotebook import tqdm as notebook_tqdm\n",
            "\n",
            "2026-01-13 19:52:43,215 - INFO:datasets:PyTorch version 2.7.0 available.\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import numpy as np\n",
        "import psutil\n",
        "import logging\n",
        "import time\n",
        "import random\n",
        "from helical.models.hyena_dna import HyenaDNA, HyenaDNAConfig"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zKUvni1mD_7R",
        "outputId": "4b0f6f14-b5a8-4834-d8ee-7ee3096a9e38"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2026-01-13 19:52:45,543 - INFO:helical.models.hyena_dna.pretrained_model:Loaded pretrained weights ok!\n",
            "2026-01-13 19:52:45,545 - INFO:helical.models.hyena_dna.model:Model finished initializing.\n",
            "2026-01-13 19:52:45,545 - INFO:helical.models.hyena_dna.model:'hyenadna-tiny-1k-seqlen-d256' model is in 'eval' mode, on device 'cpu'.\n"
          ]
        }
      ],
      "source": [
        "# Download the model using the hyena_config\n",
        "hyena_config = HyenaDNAConfig(\n",
        "    model_name = \"hyenadna-tiny-1k-seqlen-d256\"\n",
        ")\n",
        "model = HyenaDNA(configurer=hyena_config)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "7z4VrJHbD_vx",
        "outputId": "ad2f4306-a750-4876-ea5a-f95351fe09e7"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'cpu'"
            ]
          },
          "execution_count": 3,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "DEVICE"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "5X9kQ3a8anfM"
      },
      "outputs": [],
      "source": [
        "# move model to use GPU if possible\n",
        "if DEVICE == \"cuda\":\n",
        "    model.model.to(DEVICE)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 81,
          "referenced_widgets": [
            "90b2f19c56954dc3a06048d79852042d",
            "f2e2198a0b014e428aae57e67334239c",
            "2c4db2752a6b425497726e1e2317cf86",
            "b6d8ec80bbc8414b974b56d7052d85cf",
            "796af5db682b49408527e5d75450adac",
            "2c472a9566e04b07b1d0964810022a22",
            "c9b30f3dc6de4e2ca32ab7b40590fdd3",
            "dcad077b721843fabd025e1af19106dd",
            "a823c955f7b0411aad2a0d903a4f3b4d",
            "c7602a39c07843eaa75ba3af69d6fadb",
            "c453c20a0e3f4b5d8def4c9ef9bc4431",
            "715dddfbb691487192b3270d70ceb486",
            "39d989b0f6504e11b338dc7e7b2a90bb",
            "7e29a3c41cf34ac6a25837dd66559120",
            "13559fdf049c47628400f83d8989ac0c",
            "99291e8c19854c4494f3c0864d1ff9ee",
            "347b810802ef4aeaa15de9dc9309c69b",
            "4822183f0b504bc3b7de902c888f3143",
            "95832f724a4345059f2e82957824fef2",
            "688b915a56254c15a615691e6e84ebdf",
            "847ccbf4f15140deb91669f39ee709f6",
            "281b266c19bb4c32ac8defc4838c2d0f"
          ]
        },
        "id": "Lfm-tIaOD-6u",
        "outputId": "58c76b9c-48f6-4fd5-b45e-53aa7456926a"
      },
      "outputs": [],
      "source": [
        "# Download the dataset\n",
        "from datasets import load_dataset\n",
        "label = \"promoter_tata\"\n",
        "\n",
        "dataset = load_dataset(\"InstaDeepAI/nucleotide_transformer_downstream_tasks\",  trust_remote_code=True).filter(lambda x: x[\"task\"] == \"promoter_tata\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0BcnPM7aIP66",
        "outputId": "168c25e8-c4e3-4fda-f1f4-4e4891d608ed"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "DatasetDict({\n",
              "    train: Dataset({\n",
              "        features: ['sequence', 'name', 'label', 'task'],\n",
              "        num_rows: 5509\n",
              "    })\n",
              "    test: Dataset({\n",
              "        features: ['sequence', 'name', 'label', 'task'],\n",
              "        num_rows: 621\n",
              "    })\n",
              "})"
            ]
          },
          "execution_count": 6,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "AuCMiWCXIQHM"
      },
      "outputs": [],
      "source": [
        "train_dataset = dataset[\"train\"]\n",
        "test_dataset = dataset[\"test\"]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "s_ssukEXIQL2",
        "outputId": "00e71ba7-2e07-46ad-a8e2-bd642f089ce1"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "{'sequence': Value(dtype='string', id=None),\n",
              " 'name': Value(dtype='string', id=None),\n",
              " 'label': Value(dtype='int32', id=None),\n",
              " 'task': Value(dtype='string', id=None)}"
            ]
          },
          "execution_count": 8,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "train_dataset.features"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Q--VWf-mIQPG",
        "outputId": "c126ee25-d7dc-4fa6-df97-0a8532ceb1d7"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "('CGCTCCCCCAGGAGTGTACTCCTGGTCAAAAGAGCGACATCACACGACGTAGGCCCGCCCGGCTTATCGAAGTCGAGCTGGGATTTGGGGGGGAACCTGACAGTATAGGTTGGGGGCCAGGACATTTATAGAACAACGGGAAAGACCTGCGCCAGCAGCTGAGAAGGAGGCCCCGTGATCAGCTCCAGCCATTTGCCAGCAACCGAAGCCCAGGAGCTTACATAATTTGCCAGGGCAGCACTGAGAGGTGACAGTTAGAGTTAAGTCGCTCTCGGAGCTCCGGGCTACCAGCGATTCTCT',\n",
              " 300,\n",
              " 5509)"
            ]
          },
          "execution_count": 9,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "sequences = train_dataset['sequence']\n",
        "sequences[0], len(sequences[0]), len(sequences)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5R5Ix8y0M1vX",
        "outputId": "cbdcd5d2-4235-48c4-e816-8eda7a8bf66b"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(10,\n",
              " 'CGCTCCCCCAGGAGTGTACTCCTGGTCAAAAGAGCGACATCACACGACGTAGGCCCGCCCGGCTTATCGAAGTCGAGCTGGGATTTGGGGGGGAACCTGACAGTATAGGTTGGGGGCCAGGACATTTATAGAACAACGGGAAAGACCTGCGCCAGCAGCTGAGAAGGAGGCCCCGTGATCAGCTCCAGCCATTTGCCAGCAACCGAAGCCCAGGAGCTTACATAATTTGCCAGGGCAGCACTGAGAGGTGACAGTTAGAGTTAAGTCGCTCTCGGAGCTCCGGGCTACCAGCGATTCTCT')"
            ]
          },
          "execution_count": 29,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "SAMPLE_SIZE = 10\n",
        "sample_sequences = sequences[:SAMPLE_SIZE]\n",
        "len(sample_sequences), sample_sequences[0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "id": "Okwb2uPsVJ0w"
      },
      "outputs": [],
      "source": [
        "# for suppressing INFO printing on each iteration\n",
        "logging.getLogger(\"helical.models.hyena_dna.model\").setLevel(logging.WARNING)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "id": "Jz7zrrQhIQSL"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Getting embeddings: 100%|██████████| 1/1 [00:00<00:00, 19.71it/s]\n",
            "Getting embeddings: 100%|██████████| 1/1 [00:00<00:00, 35.75it/s]\n",
            "Getting embeddings: 100%|██████████| 1/1 [00:00<00:00, 30.79it/s]\n",
            "Getting embeddings: 100%|██████████| 1/1 [00:00<00:00, 34.93it/s]\n",
            "Getting embeddings: 100%|██████████| 1/1 [00:00<00:00, 38.82it/s]\n",
            "Getting embeddings: 100%|██████████| 1/1 [00:00<00:00, 29.81it/s]\n",
            "Getting embeddings: 100%|██████████| 1/1 [00:00<00:00, 30.12it/s]\n",
            "Getting embeddings: 100%|██████████| 1/1 [00:00<00:00, 35.39it/s]\n",
            "Getting embeddings: 100%|██████████| 1/1 [00:00<00:00, 30.66it/s]\n",
            "Getting embeddings: 100%|██████████| 1/1 [00:00<00:00, 30.94it/s]\n"
          ]
        }
      ],
      "source": [
        "from numpy.random.mtrand import sample\n",
        "# Inference time Without GPU (no BATCH)\n",
        "import time\n",
        "start = time.time()\n",
        "sample_embeddings = []\n",
        "latencies = []\n",
        "start_rss = psutil.Process().memory_info().rss / (1024 * 1024)\n",
        "BATCH_SIZE = 32\n",
        "overall_start = time.time()\n",
        "for i in range(SAMPLE_SIZE):\n",
        "  t_loop_in = time.time()\n",
        "  tokens = model.process_data(sample_sequences[i])\n",
        "  with torch.no_grad():\n",
        "    embeddings = model.get_embeddings(tokens)\n",
        "    sample_embeddings.append(embeddings)\n",
        "  t_loop_out = time.time()\n",
        "  latencies.append(t_loop_out - t_loop_in)\n",
        "  if isinstance(embeddings, torch.Tensor):\n",
        "    sample_embeddings.append(embeddings)\n",
        "overall_end = time.time()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {
        "id": "c0qksowYYAUa"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "------------ Profile : BATCH_SIZE = 1 ------------\n",
            "Overall time taken to inference: 0.39 s\n",
            "Avg Latency per Batch: 39.37 ms\n",
            "Throughput Processed: 25.39 samples/s\n",
            "CPU RAM Usage: 35.203125 MB\n",
            "GPU Memory Peak: 0.00 MB\n",
            "---------------------------------------------------\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Profiling (without Batching)\n",
        "overall_time = overall_end - overall_start\n",
        "avg_latency = np.mean(latencies)\n",
        "throughput = SAMPLE_SIZE/overall_time\n",
        "end_rss = psutil.Process().memory_info().rss / (1024 * 1024)\n",
        "peak_gpu = torch.cuda.max_memory_allocated() / (1024 * 1024) if torch.cuda.is_available() else 0\n",
        "\n",
        "print(f\"\"\"\n",
        "------------ Profile : BATCH_SIZE = 1 ------------\n",
        "Overall time taken to inference: {overall_time:.2f} s\n",
        "Avg Latency per Batch: {1000*avg_latency:.2f} ms\n",
        "Throughput Processed: {throughput:.2f} samples/s\n",
        "CPU RAM Usage: {end_rss - start_rss:2f} MB\n",
        "GPU Memory Peak: {peak_gpu:.2f} MB\n",
        "---------------------------------------------------\n",
        "\"\"\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {
        "id": "S5SuSoSMc0pY"
      },
      "outputs": [],
      "source": [
        "def force_move(data, device):\n",
        "    # If it has a .to() method (like Tensors or HF BatchEncoding), use it\n",
        "    if hasattr(data, \"to\"):\n",
        "        return data.to(device)\n",
        "    # If it's a dictionary, move each value manually\n",
        "    elif isinstance(data, dict):\n",
        "        return {k: force_move(v, device) for k, v in data.items()}\n",
        "    return data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {
        "id": "sSRqRxgdIQXE"
      },
      "outputs": [],
      "source": [
        "# Inference time with GPU (BATCH)\n",
        "start = time.time()\n",
        "sample_embeddings = []\n",
        "latencies = []\n",
        "start_rss = psutil.Process().memory_info().rss / (1024 * 1024)\n",
        "BATCH_SIZE = 32\n",
        "overall_start = time.time()\n",
        "for i in range(0, SAMPLE_SIZE, BATCH_SIZE):\n",
        "  t_loop_in = time.time()\n",
        "  raw_tokens = model.process_data(sample_sequences[i:i + BATCH_SIZE])\n",
        "  input_ids_tensor = torch.tensor(raw_tokens[\"input_ids\"]).to(DEVICE)\n",
        "\n",
        "  with torch.no_grad():\n",
        "    outputs = model.model(input_ids=input_ids_tensor)\n",
        "    embeddings = outputs\n",
        "  t_loop_out = time.time()\n",
        "  latencies.append(t_loop_out - t_loop_in)\n",
        "  if isinstance(embeddings, torch.Tensor):\n",
        "    sample_embeddings.append(embeddings)\n",
        "overall_end = time.time()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9zaL78LHd2Df",
        "outputId": "cd047ba5-c66a-424b-c9a4-b778b0ac4a6e"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "Dataset({\n",
              "    features: ['input_ids', 'token_type_ids', 'attention_mask'],\n",
              "    num_rows: 10\n",
              "})"
            ]
          },
          "execution_count": 35,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "raw_tokens"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uloiGu1ShPr9",
        "outputId": "cf1b722b-4dc9-4bf0-d5af-5276cafca5d0"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[tensor([[[ 0.2221, -1.1488, -0.3061,  ...,  0.1818,  0.3753,  0.3343],\n",
              "          [ 0.4377, -2.3777, -0.9752,  ..., -0.0337, -0.5294, -0.2220],\n",
              "          [ 0.3123, -2.7216, -1.0941,  ..., -1.2870, -0.4325, -0.4745],\n",
              "          ...,\n",
              "          [ 0.6696, -1.9989, -1.2394,  ..., -0.6132, -0.6580, -0.2012],\n",
              "          [ 0.7257, -2.2866, -1.3089,  ..., -1.1637, -0.5974, -0.5252],\n",
              "          [ 0.7874, -1.5952, -1.1560,  ..., -0.6187, -1.1269,  0.5691]],\n",
              " \n",
              "         [[ 0.2221, -1.1488, -0.3061,  ...,  0.1818,  0.3753,  0.3343],\n",
              "          [ 0.4377, -2.3777, -0.9752,  ..., -0.0337, -0.5294, -0.2220],\n",
              "          [ 0.3821, -2.6401, -1.1393,  ..., -0.8344, -0.4520, -0.3291],\n",
              "          ...,\n",
              "          [ 0.4790, -2.5562, -1.5859,  ..., -0.7552, -0.7874,  0.5136],\n",
              "          [ 0.4910, -2.3967, -1.1183,  ..., -0.3889, -0.7296, -0.2873],\n",
              "          [ 0.5855, -2.3533,  0.0256,  ..., -0.0341, -0.3571, -0.1109]],\n",
              " \n",
              "         [[ 0.2221, -1.1488, -0.3061,  ...,  0.1818,  0.3753,  0.3343],\n",
              "          [ 0.2844, -2.3120, -1.0185,  ..., -1.1354, -0.6762, -0.2068],\n",
              "          [ 0.5750, -2.5375, -1.0523,  ..., -0.1180, -0.3940, -0.3906],\n",
              "          ...,\n",
              "          [-0.1380, -2.5960, -0.9093,  ..., -1.1577, -0.4390, -0.4882],\n",
              "          [ 0.2323, -2.7989, -0.8363,  ..., -0.7963, -0.5778, -0.3378],\n",
              "          [ 0.5734, -2.0436, -0.5956,  ..., -0.4984, -0.2732, -0.4855]],\n",
              " \n",
              "         ...,\n",
              " \n",
              "         [[ 0.2221, -1.1488, -0.3061,  ...,  0.1818,  0.3753,  0.3343],\n",
              "          [ 0.4377, -2.3777, -0.9752,  ..., -0.0337, -0.5294, -0.2220],\n",
              "          [ 0.2963, -2.5748, -0.9303,  ..., -1.0314, -0.9281, -0.2733],\n",
              "          ...,\n",
              "          [-0.1674, -2.5763, -1.0852,  ..., -1.1988, -0.5978, -0.3038],\n",
              "          [-0.3589, -2.7225, -1.2278,  ..., -0.6167, -0.8384, -0.2868],\n",
              "          [ 0.4015, -2.7367, -1.0673,  ..., -0.3132, -0.2946, -0.1941]],\n",
              " \n",
              "         [[ 0.2221, -1.1488, -0.3061,  ...,  0.1818,  0.3753,  0.3343],\n",
              "          [ 0.2892, -2.3712, -1.2141,  ..., -0.8286, -0.5858, -0.2012],\n",
              "          [ 0.4803, -2.5965, -1.1964,  ..., -0.8623, -0.2653, -0.3432],\n",
              "          ...,\n",
              "          [-0.0627, -1.9967, -1.1648,  ..., -1.7482, -0.8552, -0.1098],\n",
              "          [ 0.0850, -2.7528, -1.0044,  ..., -0.9212, -0.4105, -0.3486],\n",
              "          [ 0.2849, -1.9055, -1.0400,  ...,  0.0536, -0.1543, -0.3062]],\n",
              " \n",
              "         [[ 0.2221, -1.1488, -0.3061,  ...,  0.1818,  0.3753,  0.3343],\n",
              "          [ 0.2844, -2.3120, -1.0185,  ..., -1.1354, -0.6762, -0.2068],\n",
              "          [ 0.2592, -2.4124, -1.0802,  ..., -1.0978, -0.5836, -0.1451],\n",
              "          ...,\n",
              "          [ 0.3217, -2.5755, -0.9897,  ..., -0.5447, -0.4211, -0.4646],\n",
              "          [-0.3134, -3.1400, -1.0956,  ..., -0.8261, -0.3248, -0.5097],\n",
              "          [ 1.0713, -1.5748,  0.3070,  ..., -0.4395, -0.0139, -1.1073]]])]"
            ]
          },
          "execution_count": 36,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "sample_embeddings"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3rAzG8Syfi5_",
        "outputId": "c557ca5f-4b28-4bda-c9aa-1059237c20b1"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "list"
            ]
          },
          "execution_count": 37,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "type(raw_tokens['input_ids'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qs32-Fn6RfHT",
        "outputId": "969a1ca6-e6ff-4daf-dbce-608e73927635"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "------------ Profile : BATCH_SIZE = 32 ------------\n",
            "Overall time taken to inference: 0.15\n",
            "Avg Latency per Batch: 149.04 ms\n",
            "Throughput Processed: 67.00 samples/s\n",
            "CPU RAM Usage: 123.375000 MB\n",
            "GPU Memory Peak: 0.00 MB\n",
            "---------------------------------------------------\n",
            "\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "import psutil\n",
        "# Profiling (without Batching)\n",
        "overall_time = overall_end - overall_start\n",
        "avg_latency = np.mean(latencies)\n",
        "throughput = SAMPLE_SIZE/overall_time\n",
        "end_rss = psutil.Process().memory_info().rss / (1024 * 1024)\n",
        "peak_gpu = torch.cuda.max_memory_allocated() / (1024 * 1024) if torch.cuda.is_available() else 0\n",
        "\n",
        "print(f\"\"\"\n",
        "------------ Profile : BATCH_SIZE = 32 ------------\n",
        "Overall time taken to inference: {overall_time:.2f}\n",
        "Avg Latency per Batch: {1000*avg_latency:.2f} ms\n",
        "Throughput Processed: {throughput:.2f} samples/s\n",
        "CPU RAM Usage: {end_rss - start_rss:2f} MB\n",
        "GPU Memory Peak: {peak_gpu:.2f} MB\n",
        "---------------------------------------------------\n",
        "\"\"\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fq9-xreSkuGZ"
      },
      "source": [
        "# Perturbation based inferencing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 45,
      "metadata": {
        "id": "WbfGhlJRksn2"
      },
      "outputs": [],
      "source": [
        "def add_pertubations(sequence_string, num_of_pertubations):\n",
        "  \"\"\"adds pertubations to a sequence of nucleotides\"\"\"\n",
        "  nucleotides = [\"A\", \"G\", \"T\", \"C\"]\n",
        "  length = len(sequence_string)\n",
        "  seq_list = list(sequence_string)\n",
        "\n",
        "  for _ in range(num_of_pertubations):\n",
        "    random_idx = np.random.randint(0, length - 1)\n",
        "\n",
        "    original_nucleotide = seq_list[random_idx]\n",
        "    possible_pertubations = [n for n in nucleotides if n != original_nucleotide]\n",
        "    new_nucleotide = random.choice(possible_pertubations)\n",
        "\n",
        "    # apply the pertubation to mutate\n",
        "    seq_list[random_idx] = new_nucleotide\n",
        "\n",
        "  # return perturbed sequence\n",
        "  return \"\".join(seq_list)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 46,
      "metadata": {
        "id": "5oaUBHGskshw"
      },
      "outputs": [],
      "source": [
        "PERTURBATION_PER_SEQUENCE = 10\n",
        "preturbed_sequences = []"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 47,
      "metadata": {
        "id": "6jo39GKwksbf"
      },
      "outputs": [],
      "source": [
        "for sequence in sample_sequences:\n",
        "    preturbed_sequences.append(add_pertubations(sequence, num_of_pertubations=1))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 48,
      "metadata": {
        "id": "NKQdVEL9ksU0"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "10"
            ]
          },
          "execution_count": 48,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# for p in preturbed_sequences[1]:\n",
        "#     print(p)\n",
        "len(preturbed_sequences)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 54,
      "metadata": {
        "id": "As4ltCk8krkw"
      },
      "outputs": [],
      "source": [
        "# Inference time with GPU (BATCH)\n",
        "start = time.time()\n",
        "perturbed_embeddings = []\n",
        "latencies = []\n",
        "start_rss = psutil.Process().memory_info().rss / (1024 * 1024)\n",
        "BATCH_SIZE = 32\n",
        "overall_start = time.time()\n",
        "for i in range(0, SAMPLE_SIZE, BATCH_SIZE):\n",
        "  t_loop_in = time.time()\n",
        "  raw_tokens = model.process_data(sample_sequences[i:i + BATCH_SIZE])\n",
        "  input_ids_tensor = torch.tensor(raw_tokens[\"input_ids\"]).to(DEVICE)\n",
        "\n",
        "  with torch.no_grad():\n",
        "    outputs = model.model(input_ids=input_ids_tensor)\n",
        "    embeddings = outputs\n",
        "  t_loop_out = time.time()\n",
        "  latencies.append(t_loop_out - t_loop_in)\n",
        "  if isinstance(embeddings, torch.Tensor):\n",
        "    perturbed_embeddings.append(embeddings)\n",
        "overall_end = time.time()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "1"
            ]
          },
          "execution_count": 56,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "perturbed_embeddings"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 58,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "------------ Profile : BATCH_SIZE = 32 ------------\n",
            "Number of Samples processed: 10\n",
            "Overall time taken to inference: 0.14\n",
            "Avg Latency per Batch: 141.59 ms\n",
            "Throughput Processed: 70.46 samples/s\n",
            "CPU RAM Usage: 83.406250 MB\n",
            "GPU Memory Peak: 0.00 MB\n",
            "---------------------------------------------------\n",
            "\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "import psutil\n",
        "# Profiling (without Batching)\n",
        "overall_time = overall_end - overall_start\n",
        "avg_latency = np.mean(latencies)\n",
        "throughput = SAMPLE_SIZE/overall_time\n",
        "end_rss = psutil.Process().memory_info().rss / (1024 * 1024)\n",
        "peak_gpu = torch.cuda.max_memory_allocated() / (1024 * 1024) if torch.cuda.is_available() else 0\n",
        "\n",
        "print(f\"\"\"\n",
        "------------ Profile : BATCH_SIZE = 32 ------------\n",
        "Number of Samples processed: {len(preturbed_sequences)}\n",
        "Overall time taken to inference: {overall_time:.2f}\n",
        "Avg Latency per Batch: {1000*avg_latency:.2f} ms\n",
        "Throughput Processed: {throughput:.2f} samples/s\n",
        "CPU RAM Usage: {end_rss - start_rss:2f} MB\n",
        "GPU Memory Peak: {peak_gpu:.2f} MB\n",
        "---------------------------------------------------\n",
        "\"\"\")"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": ".helical_conda_venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.13"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "13559fdf049c47628400f83d8989ac0c": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_847ccbf4f15140deb91669f39ee709f6",
            "placeholder": "​",
            "style": "IPY_MODEL_281b266c19bb4c32ac8defc4838c2d0f",
            "value": " 18/18 [00:00&lt;00:00, 119.46it/s]"
          }
        },
        "281b266c19bb4c32ac8defc4838c2d0f": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "2c472a9566e04b07b1d0964810022a22": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "2c4db2752a6b425497726e1e2317cf86": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_dcad077b721843fabd025e1af19106dd",
            "max": 18,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_a823c955f7b0411aad2a0d903a4f3b4d",
            "value": 18
          }
        },
        "347b810802ef4aeaa15de9dc9309c69b": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "39d989b0f6504e11b338dc7e7b2a90bb": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_347b810802ef4aeaa15de9dc9309c69b",
            "placeholder": "​",
            "style": "IPY_MODEL_4822183f0b504bc3b7de902c888f3143",
            "value": "Resolving data files: 100%"
          }
        },
        "4822183f0b504bc3b7de902c888f3143": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "688b915a56254c15a615691e6e84ebdf": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "715dddfbb691487192b3270d70ceb486": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_39d989b0f6504e11b338dc7e7b2a90bb",
              "IPY_MODEL_7e29a3c41cf34ac6a25837dd66559120",
              "IPY_MODEL_13559fdf049c47628400f83d8989ac0c"
            ],
            "layout": "IPY_MODEL_99291e8c19854c4494f3c0864d1ff9ee"
          }
        },
        "796af5db682b49408527e5d75450adac": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "7e29a3c41cf34ac6a25837dd66559120": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_95832f724a4345059f2e82957824fef2",
            "max": 18,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_688b915a56254c15a615691e6e84ebdf",
            "value": 18
          }
        },
        "847ccbf4f15140deb91669f39ee709f6": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "90b2f19c56954dc3a06048d79852042d": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_f2e2198a0b014e428aae57e67334239c",
              "IPY_MODEL_2c4db2752a6b425497726e1e2317cf86",
              "IPY_MODEL_b6d8ec80bbc8414b974b56d7052d85cf"
            ],
            "layout": "IPY_MODEL_796af5db682b49408527e5d75450adac"
          }
        },
        "95832f724a4345059f2e82957824fef2": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "99291e8c19854c4494f3c0864d1ff9ee": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "a823c955f7b0411aad2a0d903a4f3b4d": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "b6d8ec80bbc8414b974b56d7052d85cf": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_c7602a39c07843eaa75ba3af69d6fadb",
            "placeholder": "​",
            "style": "IPY_MODEL_c453c20a0e3f4b5d8def4c9ef9bc4431",
            "value": " 18/18 [00:00&lt;00:00, 104.71it/s]"
          }
        },
        "c453c20a0e3f4b5d8def4c9ef9bc4431": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "c7602a39c07843eaa75ba3af69d6fadb": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "c9b30f3dc6de4e2ca32ab7b40590fdd3": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "dcad077b721843fabd025e1af19106dd": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "f2e2198a0b014e428aae57e67334239c": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_2c472a9566e04b07b1d0964810022a22",
            "placeholder": "​",
            "style": "IPY_MODEL_c9b30f3dc6de4e2ca32ab7b40590fdd3",
            "value": "Resolving data files: 100%"
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
